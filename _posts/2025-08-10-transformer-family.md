---
title: 'The Transformer Primer'
subtitle: Scaling laws for modeling long-range dependencies (TBC)
date: 2025-08-10
permalink: /posts/transformer_family/
category: Empirics
---


This is an ongoing blog where I explore and improve my understanding of the Transformer family:

Iâ€™ll keep sharing important things I discover about Transformers over time.

### Attention


#### Self-attention


#### Multi-head attention

#### Stacking of attentions to achieve better performance via @Shuangfei Zhai's tweets.

### Masks

Casual / Chunk-based Casual/ Bi-directional

### Position embedding

Abs. PE/ Learnable P.E./ ALiBi/

Rotary Positional Encoding (RoPE has an inductive bias towards left-to-right ordering, Sitan Chen's Train for theWorst, Plan for the Best:
Understanding Token Ordering in Masked Diffusions)


