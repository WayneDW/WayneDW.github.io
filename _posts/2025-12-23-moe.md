---
title: 'Mixture of Experts: Essentials'
subtitle: A sparse routing for scalable Transformers
date: 2025-12-23
permalink: /posts/moe/
category: Transformer
---


**Mixture of experts (MoE)** scales model parameters and improves generation quality without increasing **per-token FLOPs**. Despite the name, MoE doesn't mean assembling human-like experts (e.g., finance or medicine). Instead, it replaces a big feedforward network (FFN) {% cite fedus2022switch %} {% cite deepseekv3_2024 %} {% cite openai_gptoss_github %} with many smaller blocks, where only a subset is activated for each token.


<!-- <figure style="text-align: center;">
    <img src="/images/DeepSeekMOE.png" width="434" height="244" />
    <figcaption> MoE architecture of DeepSeek-V3 {% cite deepseekv3_2024 %}. </figcaption>
</figure> -->


<div style="display: flex; justify-content: center; align-items: flex-start; gap: 24px;">
  <!-- <figure style="text-align: center; margin: 0;">
    <img src="/images/moe_switch_transformer.png" width="347" height="195" />
    <figcaption>MoE in Switch Transformer {% cite fedus2022switch %}.</figcaption>
  </figure> -->

  <!-- <figure style="text-align: center; margin: 0;">
    <img src="/images/DeepSeekMOE2.png" width="325" height="195" />
    <figcaption>MoE in DeepSeek-V3 {% cite deepseekv3_2024 %}.</figcaption>
  </figure> -->
  <figure style="text-align: center; margin: 0;">
    <img src="/images/deepseekmoe2.png" width="549" height="233" />
    <figcaption>MoE in DeepSeek-V3 {% cite deepseekv3_2024 %}.</figcaption>
  </figure>
</div>

This sparsity significantly increases model capacity while keeping computation efficient. It enables faster and more scalable training, improves utilization of available FLOPs, and supports efficient parallelism by distributing experts across different devices.


### Routing


DeepSeekMoE introduces a hybrid design that combines shared experts with a gated mixture of routed experts:

$$\begin{align}
\mathbf{h}'_t
&= \mathbf{u}_t
+ \sum_{i=1}^{N_s} \mathrm{FFN}^{(s)}_i(\mathbf{u}_t)
+ \sum_{i=1}^{N_r} g_{i,t}\,\mathrm{FFN}^{(r)}_i(\mathbf{u}_t), \notag \\[6pt]
g_{i,t}
&= \mathrm{Softmax}\!\left(\mathbf{u}_t^{\top} \mathbf{e}_i \mathrm{ \ with\ topK \ masking} \right), \notag \\
% s_{i,t}
% &= \mathrm{Sigmoid}\!\left(\mathbf{u}_t^{\top} \mathbf{e}_i\right). \notag
\end{align}$$

where $\mathbf{u_t}$ is the post-attention hidden state of the $t$-th token.

Similarly, GPT-OSS {% cite openai_gptoss_github %} adopts a closely related routing mechanism:

$$
\begin{align}
g_{i,t}
&= \mathrm{Softmax}\!\left(z_{i,t} \mathrm{ \ with\ topK \ masking} \right) \notag \\
z_{i,t}
&= \mathbf{w}_i^{\top}\,\mathrm{RMSNorm}(\mathbf{x}_t) + b_i. \notag \\
\end{align}$$

### MoE in GPT-OSS

In GPT-OSS, the MoE module is embedded directly inside the MLP block of each Transformer layer:

```python
class TransformerBlock(torch.nn.Module):
    def __init__(self, ...):
        super().__init__()
        self.layer_idx = layer_idx
        self.attn = AttentionBlock(config, layer_idx, device)
        self.mlp = MLPBlock(config, device)

    def forward(self, x):
        x = self.attn(x)
        x = self.mlp(x)
        return x
```

Below is a simplified version of GPT-OSS MoE in the MLPBlock, focusing on core routing and expert computations:

```python
class MLPBlock(nn.Module):
    def __init__(self, hidden_size, intermediate_size, num_experts, experts_per_token, swiglu_limit):
        super().__init__()
        self.num_experts = num_experts
        self.experts_per_token = experts_per_token
        self.swiglu_limit = swiglu_limit

        self.norm = RMSNorm(hidden_size)
        self.gate = nn.Linear(hidden_size, num_experts)

        # Expert parameters (maybe loaded from checkpoint)
        self.mlp1_weight = nn.Parameter(torch.empty(num_experts, 2 * intermediate_size, hidden_size))
        self.mlp1_bias   = nn.Parameter(torch.empty(num_experts, 2 * intermediate_size))
        self.mlp2_weight = nn.Parameter(torch.empty(num_experts, hidden_size, intermediate_size))
        self.mlp2_bias   = nn.Parameter(torch.empty(num_experts, hidden_size))


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        u = self.norm(x)                              
        logits = self.gate(u)                          

        topk = torch.topk(logits, k=self.experts_per_token, dim=-1, sorted=True)
        expert_idx = topk.indices                         
        expert_w   = F.softmax(topk.values, dim=-1)       

        W1 = self.mlp1_weight[expert_idx]                 # [B, K, 2M, H]
        b1 = self.mlp1_bias[expert_idx]                   # [B, K, 2M]
        h  = torch.einsum("bekh,bh->bek", W1, u) + b1     # [B, K, 2M]
        h  = swiglu(h, limit=self.swiglu_limit)           # [B, K, M]

        W2 = self.mlp2_weight[expert_idx]                 # [B, K, H, M]
        b2 = self.mlp2_bias[expert_idx]                   # [B, K, H]
        y  = torch.einsum("bekhm,bekm->bekh", W2, h) + b2 # [B, K, H]

        out = torch.einsum("bekh,bek->bh", y, expert_w)   # [B, H]
        return x + out
```


### Routing with REINFORCE


The routing decision in MoE models can be formulated as a reinforcement learning (RL) problem, where the router acts as a policy that selects a subset of experts for each token based on its hidden representation, and downstream task performance provides the reward signal {% cite clark2022unified %}. While policy-gradient methods such as REINFORCE offer a principled formulation, they suffer from high variance, slow convergence, and poor scalability in large language models. 

Alternative approaches include formulating routing as an optimal transport problem to achieve balanced expert assignment, or introducing stochastic perturbations to the gating mechanism to improve load balance and training stability.


### Load Balancing

A key challenge in MoE models is the load imbalance, where a few experts receive most tokens. To avoid mitigate this issue, a differentiable load-balancing loss is commonly added to encourage uniform utilization and stable training {% cite fedus2022switch %}.

$$
\mathrm{\mathcal{L}_{\text{lb}}
= \alpha K \sum_{k=1}^K \langle f_k, P_j \rangle,}
$$

where $\mathrm{f_k=\mathrm{Avg}(\mathbb{1}(\text{Argmax}(p(x))=k))}$ is the fraction of tokens routed to expert $k$ and $\mathrm{P_j=\mathrm{Avg}(p_k(x))}$ is the average fraction for expert $k$ in a batch.


### Fine-Tuning


During RL fine-tuning, MoE models can exhibit poor generalization due to over-specialized routing, policy collapse, or distribution shift amplified by sparse activation. Preventing overfitting in this regime remains an open problem. In practice, effective strategies include using more diverse fine-tuning data, constraining routing dynamics, and maintaining shared expert capacity.



### Citation

{% raw %}
```bibtex
@misc{deng2025MoE,
  title   ={{Mixture of Experts: Essentials}},
  author  ={Wei Deng},
  journal ={waynedw.github.io},
  year    ={2025},
  howpublished = {\url{https://weideng.org/posts/moe}}
}
```
{% endraw %}
