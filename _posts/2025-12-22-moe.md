---
title: 'Mixture of Experts: Essentials'
subtitle: Sparse routing for scalable Transformers
date: 2025-12-22
permalink: /posts/moe/
category: Transformer
---


**Mixture of experts (MoE)** scales model parameters and improves generation quality without increasing **per-token FLOPs**. Despite the name, MoE doesn't mean assembling human-like experts (e.g., finance or medicine). Instead, it replaces a big transformer {% cite deepseekv3_2024 %} or MLP block {% cite fedus2022switch %} with many smaller blocks, where only a subset is activated for each token.


<figure style="text-align: center;">
    <img src="/images/DeepSeekMOE.png" width="434" height="244" />
    <figcaption> MoE architecture of DeepSeek-V3 {% cite deepseekv3_2024 %}. </figcaption>
</figure>

This sparsity significantly increases model capacity while keeping computation efficient. It enables faster and more scalable training, improves utilization of available FLOPs, and supports efficient parallelism by distributing experts across different devices.

<!-- 
### xxxx


### Learning to Route

routing func, survey 2022 fedus
almost all do: Token â†’ choose experts

RL to learn routing, too prohibitive

optimal transport problem using MoE




training obj

RL: the most principled way, non-differentiable routing decision, think it as a policy. 

reinforce baseline approach clark 2020, not much better than Hash
SA more scalable, perturbations. still do topK, softmax gate, add noise for stochastic exploration, 

stochastic jitter, Fedus et al 2022, 

heuristic balancing : how to use expert evenly, 


z - loss stability for the router

fine-tuning: can overfit on smaller fine-tuning data

{% cite fedus2022switch %}

{% cite deepseekv3_2024 %}

DeepSeek use more data

GPS-OSS

<br><br><br> -->

### Citation

{% raw %}
```bibtex
@misc{deng2025MoE,
  title   ={{Mixture of Experts: Essentials}},
  author  ={Wei Deng},
  journal ={waynedw.github.io},
  year    ={2025},
  howpublished = {\url{https://weideng.org/posts/moe}}
}
```
{% endraw %}
