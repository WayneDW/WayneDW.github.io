---
title: 'Dynamic Importance Sampling'
date: 2020-11-05
permalink: /posts/CSGLD/
tags:
  - importance sampling
  - uncertainty quantification
  - mean field
  - flattened distribution
  - stochastic approximation
  - weighted ensemble
---

Point estimation tends to over-predict out-of-distribution samples [[1]](https://arxiv.org/pdf/1612.01474.pdf) and leads to unreliable predictions. Given a cat-dog classifier, can we predict flamingo as the **unknown** class?

<p align="center">
    <img src="/images/cat_dog.png" width="500" />
</p>

The key to answering this question is **uncertainty**, which is still an open question. Yarin gave a good tutorial on uncertainty predictions using Dropout [[2]](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html). However, that method tends to underestimate uncertainty due to the nature of variational inference. 

## Importance sampling
How can we give efficient uncertainty quantification for deep neural network? Suppose we are interested in a Gaussian mixture distribution, the standard stochastic gradient Langevin dynamics ([SGLD](https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf)) suffers from the local trap issue.

<p align="center">
    <img src="/images/original_density.png" width="250" height="250" />
</p>


To tackle that issue and acceleration computations, we consider importance sampling 

<p align="center">
    <img src="/images/importance_sampling.png" width="600" height="200" />
</p>

That is when the original density is hard to simulate, but the new density is easier. Together with the importance weight, we can obtain an estimate indirectly by sampling from a new density. 

## Build a flattened density

What kind of distribution is easier than the original? A **flattened** distribution!

<p align="center">
    <img src="/images/flat_density.png" width="300" height="300" />
</p>

How to build such a flat density? We exploit ideas from [Wang-Landau algorithm](https://en.wikipedia.org/wiki/Wang_and_Landau_algorithm) and divide the original density by the **energy PDF**. 
<p align="center">
    <img src="/images/energyPDF.png" width="600" height="100" />
</p>

Given the energy PDF, we can enjoy a **random walk** in the **energy space**. Moreover, the bias caused by simulating from a different density can be adjusted by the importance weight. The following is an example [\[code\]](https://github.com/WayneDW/Contour-Stochastic-Gradient-Langevin-Dynamics/blob/master/CSGLD_demo.ipynb) to show what the flattened and reweigted densities look like.

<p align="center">
    <img src="/images/resample.png" width="700" height="200" title="A mixture example with 9 modes" />
</p>

## Estimate the energy PDF via stochastic approximation
Since we donâ€™t know the energy PDF in the beginning, we can adaptively estimate it on the fly via **stochastic approximation**. In the long run, we expect that the energy PDF is gradually estimated and we can eventually simulate from the target flat density. Theoretically, this algorithm has a stability property such that the **estimate of energy PDF converges to a unique fixed point regardless of the non-convexity** of the energy function. 

The following is a demo to show how the energy PDF is estimated. In the beginning, CSGLD behaves similar to SGLD. But soon enough, it moves quite **freely** in the energy space.

<p float="left" align="center">
  <img src="/images/CSGLD/CSGLD_with_PDF.gif" width="200" title="SGLD"/>
  <img src="/images/CSGLD/CSGLD_PDF.gif" width="200" alt="Made with Angular" title="Angular" /> 
</p>

## Sample trajectory
CSGLD possesses a self-adjusting mechanism to escape local traps. Most notably, it leads to **smaller or even negative learning rates in low energy regions to bounce particles out**. 

<p align="center">
    <img src="/images/moves.png" width="700" height="200" />
</p>

## Comparison with other methods
We compare CSGLD with some state-of-art sampling algorithms, [SGLD](https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf), [cycSGLD](https://arxiv.org/pdf/1902.03932.pdf), [reSGLD](https://arxiv.org/pdf/2008.05367.pdf) and observe that CSGLD is comparable to reSGLD and faster than SGLD and cycSGLD.
<p float="left">
  <img src="/images/CSGLD/SGLD.gif" width="185" title="SGLD"/>
  <img src="/images/CSGLD/cycSGLD.gif" width="185" alt="Made with Angular" title="Angular" />
  <img src="/images/CSGLD/reSGLD.gif" width="185" alt="hello!" title="adam solomon's hello"/>
  <img src="/images/CSGLD/CSGLD.gif" width="185" />
</p>

| Methods   |      Speed      | Special features  | Cost |
|----------|:-------------:|:-------------:|:-------------:|
| SGLD (ICML'11) |  Extremely slow | None | None |
| Cycic SGLD (ICLR'20) |    Medium   | Cyclic learning rates  | More cycles |
| Replica exchange SGLD (ICML'20) | Fast | Swaps/Jumps | Parallel chains |
| Contour SGLD (NeurIPS'20) | Fast | Bouncy moves | Latent vector |

## Summary
This algorithm can be viewed as a scalable Wang-Landau algorithm in deep learning. It paves the way for future research in various dynamic importance samplers and adaptive biasing force (ABF) techniques for big data problems. We are working on lots of extensions of this algorithm and thanks for your interest. If you like this paper, please cite

```
@inproceedings{CSGLD,
  title={A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions},
  author={Wei Deng and Guang Lin and Faming Liang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
```


## References:

1. [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble](https://arxiv.org/pdf/1612.01474.pdf). NeurIPS'17.

2. [What My Deep Model Doesn't Know and Why Should I Care About Uncertainty?](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html)

3. [Bayesian Learning via Stochastic Gradient Langevin Dynamics](https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf). ICML'11

4. [Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning](https://arxiv.org/pdf/1902.03932.pdf). ICLR'20

5. [Non-convex Learning via Replica Exchange Stochastic Gradient MCMC](https://arxiv.org/pdf/2008.05367.pdf). ICML'20.

6. [A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions](https://arxiv.org/pdf/2010.09800.pdf). NeurIPS'20.

