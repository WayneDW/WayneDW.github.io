---
title: 'Dynamic Importance Sampling'
date: 2020-11-05
permalink: /posts/2020/12/blog-post-4/
tags:
  - importance sampling
  - uncertainty quantification
  - mean field
  - flattened distribution
  - stochastic approximation
  - weighted ensemble
---

Point estimation in deep learning tends to over-predict results [[1]](https://arxiv.org/pdf/1612.01474.pdf) and leads to unreliable predictions for out-of-distribution samples. Suppose we are given a cat-dog classifier. Can we predict flamingo as the **unknown** class?

<p align="center">
    <img src="/images/cat_dog.png" />
</p>

The key to answering this question is **uncertainty**. However, it is still an open question. Yarin already gives a good tutorial on uncertainty predictions using Dropout [[2]](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html). However, that method tends to underestimate the uncertainty due to the nature of variational inference. 


How can we give accurate uncertainty quantification for deep learning? Suppose we are interested in a Gaussian mixture distribution, the standard stochastic gradient Langevin dynamics ([SGLD](https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf)) will suffer from the local trap issue.

<p align="center">
    <img src="/images/original_density.png" width="200" height="200" />
</p>


To alleviate the local trap problem and **accelerate** the simulations, we consider to simulate from a **flattened** distribution

<p align="center">
    <img src="/images/flat_density.png" width="300" height="300" />
</p>

How to build the flat density? We exploit ideas from [Wang-Landau algorithm](https://en.wikipedia.org/wiki/Wang_and_Landau_algorithm). That is we divide the original density by the **energy PDF**. If we know the energy PDF, we simulate from the desired flat density and have a **random walk in energy space**. Moreover, the bias caused by simulating from a different target can be adjusted by the importance weight, which is also the energy PDF.

<p align="center">
    <img src="/images/resample.png" width="800" height="200" title="A mixture example with 9 modes" />
</p>

Since we donâ€™t know the energy PDF in the beginning, we can adaptively estimate it on the fly via **stochastic approximation**. In the long run, we expect that theta is gradually estimated and we can eventually simulate from the target flat density. The following is a demo to show how the energy PDF is estimated. In the beginning, CSGLD behaves similar to SGLD. But soon enough, it moves quite **freely** in the energy space.

<p float="left" align="center">
  <img src="/images/CSGLD/CSGLD_with_PDF.gif" width="200" title="SGLD"/>
  <img src="/images/CSGLD/CSGLD_PDF.gif" width="200" alt="Made with Angular" title="Angular" /> 
</p>

To summarize our method, our method induces an adaptive learning rate schedule to escape local traps. Most notably, it leads to **smaller or even negative learning rates in low energy regions to bounce out of local regions**. 

<p align="center">
    <img src="/images/moves.png" width="600" height="200" />
</p>


We also compare our methods with some state-of-art sampling algorithms, [SGLD](https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf), [cycSGLD](https://arxiv.org/pdf/1902.03932.pdf), [reSGLD](https://arxiv.org/pdf/2008.05367.pdf) and observe that CSGLD is comparable to reSGLD and much faster than SGLD and cycSGLD.
<p float="left">
  <img src="/images/CSGLD/SGLD.gif" width="185" title="SGLD"/>
  <img src="/images/CSGLD/cycSGLD.gif" width="185" alt="Made with Angular" title="Angular" />
  <img src="/images/CSGLD/reSGLD.gif" width="185" alt="hello!" title="adam solomon's hello"/>
  <img src="/images/CSGLD/CSGLD.gif" width="185" />
</p>

| Methods   |      Speed      | Special features  | Cost |
|----------|:-------------:|:-------------:|:-------------:|
| SGLD (ICML'11) |  Extremely slow | None | None |
| Cycic SGLD (ICLR'20) |    Medium   | Cyclic learning rates  | More cycles |
| Replica exchange SGLD (ICML'20) | Fast | Swaps/Jumps | Parallel chains |
| Contour SGLD (NeurIPS'20) | Fast | Bouncy moves | Latent vector |




## References:

1. [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble](https://arxiv.org/pdf/1612.01474.pdf). NeurIPS'17.

2. [What My Deep Model Doesn't Know and Why Should I Care About Uncertainty?](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html)

3. [Bayesian Learning via Stochastic Gradient Langevin Dynamics](https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf). ICML'11

4. [Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning](https://arxiv.org/pdf/1902.03932.pdf). ICLR'20

5. [Non-convex Learning via Replica Exchange Stochastic Gradient MCMC](https://arxiv.org/pdf/2008.05367.pdf). ICML'20.

6. [A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions](https://arxiv.org/pdf/2010.09800.pdf). NeurIPS'20.

