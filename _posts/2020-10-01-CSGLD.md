---
title: 'Dynamic Importance Sampling'
date: 2020-12-01
permalink: /posts/2020/12/blog-post-4/
tags:
  - importance sampling
  - uncertainty quantification
  - mean field
  - flattened distribution
---

Point estimation in deep learning tends to over-predict results [1]. It gives good predictions if the testing data is similar to the training data but fails for out-of-distribution samples.

Suppose we are given a cat-dog classifier. 

![Over-prediction in deep learning](/images/CSGLD/cat_dog.png)

![Over-prediction in deep learning](/images/cat_dog.png)

What the prediction will be for an input of flamingo?


A reliable model should not only make the right decision among potential candidates but also cast doubts on irrelevant choices.


The key to solving that problem is quantify uncertainty efficiently and correctly. Uncertainty quantification for unimodal distribution is a standard result. However, it is still an ongoing topic in deep learning. Regarding efficiency, Yarin already gives a pretty good tutorial talking about uncertainty predictions using Dropout. [2]. However, that method belongs to the class of variance inference, which tend to underestimate the uncertainty. 


So how can we give a more correct uncertainty quantification for deep learning? Suppose we are interested in the simulation of a Gaussian mixture distribution, the standard stochastic gradient Langevin dynamics will suffer from the local trap issue and spends too much time in a local region.  

![Energy barrier](/images/CSGLD/original_density.png)

To alleviate the local trap problem, we may consider to simulate from a flattened distribution

![Energy barrier](/images/CSGLD/flat_density.png)


Where does the flat density comes from?

We exploit ideas from the Wang-Landau algorithm. That is




If we know theta_{\star}, then we can simulate from a flat density, so that the simulation can be much accelerated. Moreover, theta acts as the importance weight. By reweighting the samples, we can recover the original density.


How to learn theta? We donâ€™t know it in the beginning, but we can adaptively estimate theta on the fly. That is we simulate the parameter and adaptively estimate theta, in the long run, we expect that theta is gradually estimated and we can eventually simulate from the target flat density.




[1] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble. In Advances in Neural Information Processing Systems (NeurIPS), 2017.
[2] Yarin Gal. What My Deep Model Doesn't Know and Why Should I Care About Uncertainty?. https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html
