---
title: 'Dynamic Importance Sampling and Beyond'
subtitle: Negative learning rates help escape local traps.
layout: default
date: 2020-11-05
permalink: /posts/CSGLD/
published: true
category: Monte Carlo
---

Point estimation tends to over-predict out-of-distribution samples {% cite Balaji17 %} and leads to unreliable predictions. Given a cat-dog classifier, can we predict flamingo as the **unknown** class?

<p align="center">
    <img src="/images/cat_dog.png" width="400" />
</p>

The key to answering this question is **uncertainty**, which is still an open question. Yarin gave a good tutorial on uncertainty predictions using Dropout {% cite YARIN_GAL_blog %}. However, that method tends to underestimate uncertainty due to the nature of variational inference. 

## Importance sampling
How can we give efficient uncertainty quantification for deep neural networks? To answer this question, we first show a baby example. Suppose we are interested in a Gaussian mixture distribution, the standard stochastic gradient Langevin dynamics {% cite Welling11 %} suffers from the local trap issue.

<p align="center">
    <img src="/images/original_density.png" width="250" height="250" />
</p>


To tackle that issue and accelerate computations, we consider importance sampling 

<p align="center">
    <img src="/images/importance_sampling.png" width="600" height="90" />
</p>

That is when the original density is hard to simulate, but the new density is easier. Together with the importance weight, we can obtain an estimate indirectly by sampling from a new density. 

## Build a flattened density

What kind of distribution is easier than the original? A **flattened** distribution!

<p align="center">
    <img src="/images/flat_density.png" width="300" height="300" />
</p>

How to build such a flat density? One famous example is [annealed importance sampling](https://arxiv.org/pdf/physics/9803008.pdf) via high temperatures; another (ours) is to exploit ideas from [Wang-Landau algorithm](https://en.wikipedia.org/wiki/Wang_and_Landau_algorithm) and divide the original density by the **energy PDF**. 
<p align="center">
    <img src="/images/energyPDF.png" width="600" height="60" />
</p>

Given the energy PDF, we can enjoy a **random walk** in the **energy space**. Moreover, the bias caused by simulating from a different density can be adjusted by the importance weight.

## Sample trajectory in terms of learning rates
CSGLD possesses a self-adjusting mechanism to escape local traps. Most notably, it leads to **smaller or even negative learning rates in low energy regions to bounce particles out**.

<p align="center">
    <img src="/images/moves.png" width="700" height="200" />
</p>


## Estimate the energy PDF via stochastic approximation
Since we don’t know the energy PDF in the beginning, we can adaptively estimate it on the fly via **stochastic approximation**. In the long run, we expect that the energy PDF is gradually estimated and we can eventually simulate from the target flat density. Theoretically, this algorithm has a stability property such that the **estimate of energy PDF converges to a unique fixed point regardless of the non-convexity** of the energy function. 

The following is a demo to show how the energy PDF is estimated. In the beginning, CSGLD behaves similarly to SGLD. But soon enough, it moves quite **freely** in the energy space.

<p float="left" align="center">
  <img src="/images/CSGLD/CSGLD_with_PDF.gif" width="200" title="SGLD"/>
  <img src="/images/CSGLD/CSGLD_PDF.gif" width="200" alt="Made with Angular" title="Angular" /> 
</p>

The following result shows [\[code\]](https://github.com/WayneDW/Contour-Stochastic-Gradient-Langevin-Dynamics/blob/master/CSGLD_demo.ipynb) what the flattened and reweighted densities look like.

<p align="center">
    <img src="/images/resample.png" width="600" height="210" title="A mixture example with 9 modes" />
</p>

## Comparison with other methods
We compare CSGLD {% cite CSGLD %} with SGLD {% cite Welling11 %}, {% cite ruqi2020 %}, and {% cite deng2020 %}, and observe that CSGLD is comparable to reSGLD and faster than SGLD and cycSGLD.
<p float="left">
  <img src="/images/CSGLD/SGLD.gif" width="170" title="SGLD"/>
  <img src="/images/CSGLD/cycSGLD.gif" width="170" alt="Made with Angular" title="Angular" />
  <img src="/images/CSGLD/reSGLD.gif" width="170" alt="hello!" title="adam solomon's hello"/>
  <img src="/images/CSGLD/CSGLD.gif" width="170" />
</p>

| Methods   |    Special features  | Cost |
|---------------------------------|:-----------------------:|:---------------------:|
| SGLD (ICML'11) |  None | None |
| Cycic SGLD (ICLR'20) |   Cyclic learning rates  | More cycles |
| Replica exchange SGLD (ICML'20) | Swaps/Jumps | Parallel chains |
| Contour SGLD (NeurIPS'20) | Bouncy moves | Latent vector |

## Summary
Contour SGLD can be viewed as a scalable Wang-Landau algorithm in deep learning. It paves the way for future research in various adaptive biasing force techniques for big data problems. We are working on extensions of this algorithm in both theory and large-scale AI applications. If you like this paper, you can cite

```
@inproceedings{CSGLD,
  title={A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions},
  author={Wei Deng and Guang Lin and Faming Liang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
```

For Chinese readers, you may also find this blog interesting [知乎](https://zhuanlan.zhihu.com/p/267633636).

