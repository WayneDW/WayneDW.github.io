I"¶<p>Variance-reduced sampling algorithms [1,2] are not widely adopted in practice. Alternatively, we focus on the energy variance reduction to exploit exponential accelerations but no longer consider the gradient variance reduction.</p>

<p>To this end, we consider a standard sampling algorithm, the stochastic gradient Langevin dynamics (SGLD), which is a mini-batch numerical discretization of a stochastic differential equation (SDE) as follows:</p>

<p>\begin{equation}
\beta_{k+1}=\beta_k - \eta \frac{N}{n}\nabla \sum_{i\in B} L(x_i|\beta_k) + \sqrt{2\eta\tau_1} \xi_k,
\end{equation}</p>

<p>where $\beta\in\mathbb{R}^d$, $L(x_i|\beta)$ is the energy function based on the i-th data point and B denotes a data set of size $n$ simulated from the whole data of size $N$. $\xi$ is a d-dimensional Gaussian vector. It is known that a non-convex $U(\cdot)$ often leads to an exponentially slow mixing rate.</p>

<!--- Simulated annealing is adopted in almost every espect in deep learning, which proposes to anneal temperatures to concentrate the probability measures towards the global optima. Such a strategy, however, fails in uncertainty estimations for reliable predictions. -->

<p>To accelerate the simulations, replica exchange proposes to run multiple stochastic processes with different temperatures, where interactions between different SGLD chains are conducted in a manner that encourages large jumps.</p>

<!--- The following is a figure that shows the trajectory of the algorithm, where the right path denotes a process driven by a high temperature and the blue one denotes a low-temperature process. -->

<p align="center">
    <img src="/images/reSGLD_exploitation_exploration.png" width="500" height="250" />
</p>

<p>In particular, the parameters swap the positions with a probability $1\wedge S(\beta^{(1)}, \beta^{(2)})$</p>

<p>\begin{equation}
S(\beta^{(1)}, \beta^{(2)})=e^{\left(\frac{1}{\tau_1}-\frac{1}{\tau_2}\right)\left(\frac{N}{n}\sum_{i\in B} L(x_i|\beta^{(1)})-\frac{N}{n}\sum_{i\in B} L(x_i|\beta^{(2)})-(\frac{1}{\tau_1}-\frac{1}{\tau_2})\sigma^2\right)},
\end{equation}</p>

<p>where $\sigma^2$ is the variance of the noisy estimators $\sum_{i\in B} L(x_i|\cdot)$. Under Normality assumptions, the above rule leads to an unbiased swapping probability, which satisfy the detailed balance in a stochastic sense. However, the efficiency of the swaps are significantly reduced due to the requirement of corrections to avoid biases.</p>

<p>The desire to obtain more effective swaps drives us to design more efficient energy estimators. To reduce the variance of the noisy energy estimator $L(B|\beta^{(h)})=\frac{N}{n}\sum_{i\in B}L(x_i|\beta^{(h)})$ for $h\in{1,2}$, we consider an unbiased estimator $L(B|\widehat\beta^{(h)})$ for $\sum_{i=1}^N L(x_i|\widehat\beta^{(h)})$ and a constant $c$, we see that a new estimator $\widetilde L(B| \beta^{(h)})$, which follows
\begin{equation}
    \widetilde L(B|\beta^{(h)})= L(B|\beta^{(h)}) +c\left( L(B|\widehat\beta^{(h)}) -\sum_{i=1}^N L (x_i| \widehat \beta^{(h)})\right),
\end{equation}
is still the unbiased estimator for $\sum_{i=1}^N L(x_i| \beta^{(h)})$. Moreover, energy variance reduction potentially increases the swapping efficiency exponentially given a larger batch size $n$, a small learning rate $\eta$, and a more frequent update of control variate $\widehat \beta$, i.e. a small $m$</p>

<p>\begin{equation}
Var\left(\widetilde L(B|\beta^{(h)})\right)\leq O\left(\frac{m^2 \eta}{n}\right).
\end{equation}</p>

<p>The following shows a demo that explains how variance-reduced reSGLD works.</p>

<p float="left" align="center">
  <img src="/images/VR-reSGLD/SGLD.gif" width="185" title="SGLD" />
  <img src="/images/VR-reSGLD/reSGLD_vs_VR_reSGLD.gif" width="340" alt="Made with Angular" title="reSGLD vs VR-reSGLD" />
</p>

<h2 id="references">References:</h2>

<ol>
  <li>Dubey, Reddi, Poczos, Smola, Xing, Williamson. Variance Reduction in Stochastic Gradient Langevin Dynamics. NeurIPSâ€™16.</li>
  <li>Xu, Chen, Zou, Gu. Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization. NeurIPSâ€™18.</li>
  <li>Chen, Chen, Dong, Peng, Wang. <a href="https://arxiv.org/pdf/2007.01990.pdf">Accelerating Nonconvex Learning via Replica Exchange Langevin Diffusion</a>. ICLRâ€™19.</li>
  <li>Deng, Feng, Gao, Liang, Lin. <a href="https://arxiv.org/pdf/2010.01084.pdf">Non-Convex Learning via Replica Exchange Stochastic Gradient MCMC</a>. ICMLâ€™20.</li>
  <li>Deng, Feng, Karagiannis, Lin, Liang. <a href="https://arxiv.org/pdf/2010.01084.pdf">Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction</a>. ICLRâ€™21.</li>
  <li>George Yin and Chao Zhu. Hybrid Switching Diffusions: Properties and Applications. Springer, 2010.</li>
</ol>
:ET