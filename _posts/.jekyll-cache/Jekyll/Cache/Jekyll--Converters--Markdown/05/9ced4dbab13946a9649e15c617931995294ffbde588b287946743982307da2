I"Ú<p>Point estimation tends to over-predict out-of-distribution samples <a href="https://arxiv.org/pdf/1612.01474.pdf">[1]</a> and leads to unreliable predictions. Given a cat-dog classifier, can we predict flamingo as the <strong>unknown</strong> class?</p>

<p align="center">
    <img src="/images/cat_dog.png" width="400" />
</p>

<p>The key to answering this question is <strong>uncertainty</strong>, which is still an open question. Yarin gave a good tutorial on uncertainty predictions using Dropout <a href="https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html">[2]</a>. However, that method tends to underestimate uncertainty due to the nature of variational inference.</p>

<h2 id="importance-sampling">Importance sampling</h2>
<p>How can we give efficient uncertainty quantification for deep neural networks? To answer this question, we first show a baby example. Suppose we are interested in a Gaussian mixture distribution, the standard stochastic gradient Langevin dynamics (<a href="https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf">SGLD</a>) suffers from the local trap issue.</p>

<p align="center">
    <img src="/images/original_density.png" width="250" height="250" />
</p>

<p>To tackle that issue and accelerate computations, we consider importance sampling</p>

<p align="center">
    <img src="/images/importance_sampling.png" width="600" height="90" />
</p>

<p>That is when the original density is hard to simulate, but the new density is easier. Together with the importance weight, we can obtain an estimate indirectly by sampling from a new density.</p>

<h2 id="build-a-flattened-density">Build a flattened density</h2>

<p>What kind of distribution is easier than the original? A <strong>flattened</strong> distribution!</p>

<p align="center">
    <img src="/images/flat_density.png" width="300" height="300" />
</p>

<p>How to build such a flat density? One famous example is <a href="https://arxiv.org/pdf/physics/9803008.pdf">annealed importance sampling</a> via high temperatures; another (ours) is to exploit ideas from <a href="https://en.wikipedia.org/wiki/Wang_and_Landau_algorithm">Wang-Landau algorithm</a> and divide the original density by the <strong>energy PDF</strong>.</p>
<p align="center">
    <img src="/images/energyPDF.png" width="600" height="60" />
</p>

<p>Given the energy PDF, we can enjoy a <strong>random walk</strong> in the <strong>energy space</strong>. Moreover, the bias caused by simulating from a different density can be adjusted by the importance weight.</p>

<h2 id="sample-trajectory-in-terms-of-learning-rates">Sample trajectory in terms of learning rates</h2>
<p>CSGLD possesses a self-adjusting mechanism to escape local traps. Most notably, it leads to <strong>smaller or even negative learning rates in low energy regions to bounce particles out</strong>.</p>

<p align="center">
    <img src="/images/moves.png" width="700" height="200" />
</p>

<h2 id="estimate-the-energy-pdf-via-stochastic-approximation">Estimate the energy PDF via stochastic approximation</h2>
<p>Since we donâ€™t know the energy PDF in the beginning, we can adaptively estimate it on the fly via <strong>stochastic approximation</strong>. In the long run, we expect that the energy PDF is gradually estimated and we can eventually simulate from the target flat density. Theoretically, this algorithm has a stability property such that the <strong>estimate of energy PDF converges to a unique fixed point regardless of the non-convexity</strong> of the energy function.</p>

<p>The following is a demo to show how the energy PDF is estimated. In the beginning, CSGLD behaves similarly to SGLD. But soon enough, it moves quite <strong>freely</strong> in the energy space.</p>

<p float="left" align="center">
  <img src="/images/CSGLD/CSGLD_with_PDF.gif" width="200" title="SGLD" />
  <img src="/images/CSGLD/CSGLD_PDF.gif" width="200" alt="Made with Angular" title="Angular" /> 
</p>

<p>The following result shows <a href="https://github.com/WayneDW/Contour-Stochastic-Gradient-Langevin-Dynamics/blob/master/CSGLD_demo.ipynb">[code]</a> what the flattened and reweighted densities look like.</p>

<p align="center">
    <img src="/images/resample.png" width="600" height="210" title="A mixture example with 9 modes" />
</p>

<h2 id="comparison-with-other-methods">Comparison with other methods</h2>
<p>We compare CSGLD with <a href="https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf">SGLD</a>, <a href="https://arxiv.org/pdf/1902.03932.pdf">cycSGLD</a>, and <a href="https://arxiv.org/pdf/2008.05367.pdf">reSGLD</a>, and observe that CSGLD is comparable to reSGLD and faster than SGLD and cycSGLD.</p>
<p float="left">
  <img src="/images/CSGLD/SGLD.gif" width="170" title="SGLD" />
  <img src="/images/CSGLD/cycSGLD.gif" width="170" alt="Made with Angular" title="Angular" />
  <img src="/images/CSGLD/reSGLD.gif" width="170" alt="hello!" title="adam solomon's hello" />
  <img src="/images/CSGLD/CSGLD.gif" width="170" />
</p>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th style="text-align: center">Special features</th>
      <th style="text-align: center">Cost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SGLD (ICMLâ€™11)</td>
      <td style="text-align: center">None</td>
      <td style="text-align: center">None</td>
    </tr>
    <tr>
      <td>Cycic SGLD (ICLRâ€™20)</td>
      <td style="text-align: center">Cyclic learning rates</td>
      <td style="text-align: center">More cycles</td>
    </tr>
    <tr>
      <td>Replica exchange SGLD (ICMLâ€™20)</td>
      <td style="text-align: center">Swaps/Jumps</td>
      <td style="text-align: center">Parallel chains</td>
    </tr>
    <tr>
      <td>Contour SGLD (NeurIPSâ€™20)</td>
      <td style="text-align: center">Bouncy moves</td>
      <td style="text-align: center">Latent vector</td>
    </tr>
  </tbody>
</table>

<h2 id="summary">Summary</h2>
<p>Contour SGLD can be viewed as a scalable Wang-Landau algorithm in deep learning. It paves the way for future research in various adaptive biasing force techniques for big data problems. We are working on extensions of this algorithm in both theory and large-scale AI applications. If you like this paper, you can cite</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{CSGLD,
  title={A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions},
  author={Wei Deng and Guang Lin and Faming Liang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
</code></pre></div></div>

<p>For Chinese readers, you may also find this blog interesting <a href="https://zhuanlan.zhihu.com/p/267633636">çŸ¥ä¹Ž</a>.</p>

<h2 id="references">References:</h2>

<ol>
  <li>
    <p><a href="https://arxiv.org/pdf/1612.01474.pdf">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble</a>. NeurIPSâ€™17.</p>
  </li>
  <li>
    <p><a href="https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html">What My Deep Model Doesnâ€™t Know and Why Should I Care About Uncertainty?</a></p>
  </li>
  <li>
    <p><a href="https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf">Bayesian Learning via Stochastic Gradient Langevin Dynamics</a>. ICMLâ€™11</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1902.03932.pdf">Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning</a>. ICLRâ€™20</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2008.05367.pdf">Non-convex Learning via Replica Exchange Stochastic Gradient MCMC</a>. ICMLâ€™20.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2010.09800.pdf">A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions</a>. NeurIPSâ€™20.</p>
  </li>
</ol>

:ET