---
title: 'Exponential Acceleration via Replica Exchange and Beyond'
date: 2021-05-01
permalink: /posts/replica/
tags:
  - replica exchange
  - parallel tempering
  - simulations of multi-modal distributions
  - variance reduction
---

A long standing technique for variance reduction is the control variates method. Despite the advantages in near-convex problems for the gradient-based variance reduction, a natural discrepancy between theory and practice is whether we should avoid the gradient noise in non-convex problems. To fill in the gap, we only focus on the energy variance reduction to exploit exponential accelerations but no longer consider the gradient variance reduction to inherit from empirical experience.

<p float="left" align="center">
  <img src="/images/VR-reSGLD/SGLD.gif" width="185" title="SGLD"/>
  <img src="/images/VR-reSGLD/reSGLD_vs_VR_reSGLD.gif" width="340" alt="Made with Angular" title="reSGLD vs VR-reSGLD" />
</p>

Coming soon

## References:

1. Deng, Feng, Karagiannis, Lin, Liang. Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction. ICLR'21.
2. Xu, Chen, Zou, Gu. Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization. NeurIPS'18.
3. Dubey, Reddi, Poczos, Smola, Xing, Williamson. Variance Reduction in Stochastic Gradient Langevin Dynamics. NeurIPS'16.
4. George Yin and Chao Zhu. Hybrid Switching Diffusions: Properties and Applications. Springer, 2010.
5. Chen, Chen, Dong, Peng, Wang. Accelerating Nonconvex Learning via Replica Exchange Langevin Diffusion. ICLR'19.
6. Deng, Feng, Gao, Liang, Lin. Non-Convex Learning via Replica Exchange Stochastic Gradient MCMC. ICML'20.
