---
title: 'Exponential Acceleration via Replica Exchange and Beyond'
date: 2021-05-01
permalink: /posts/replica/
tags:
  - replica exchange
  - parallel tempering
  - simulations of multi-modal distributions
  - variance reduction
---

Despite the popularity for the gradient-based variance reduction [4,5], a discrepancy between theory and practice is whether we should avoid the gradient noise in non-convex problems. To fill in the gap, we only focus on the energy variance reduction to exploit exponential accelerations but no longer consider the gradient variance reduction to inherit from empirical experience.

<p float="left" align="center">
  <img src="/images/VR-reSGLD/SGLD.gif" width="185" title="SGLD"/>
  <img src="/images/VR-reSGLD/reSGLD_vs_VR_reSGLD.gif" width="340" alt="Made with Angular" title="reSGLD vs VR-reSGLD" />
</p>

Coming soon

## References:

1. Chen, Chen, Dong, Peng, Wang. Accelerating Nonconvex Learning via Replica Exchange Langevin Diffusion. ICLR'19.
2. Deng, Feng, Gao, Liang, Lin. Non-Convex Learning via Replica Exchange Stochastic Gradient MCMC. ICML'20.
3. Deng, Feng, Karagiannis, Lin, Liang. Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction. ICLR'21.
4. Dubey, Reddi, Poczos, Smola, Xing, Williamson. Variance Reduction in Stochastic Gradient Langevin Dynamics. NeurIPS'16.
5. Xu, Chen, Zou, Gu. Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization. NeurIPS'18.
6. George Yin and Chao Zhu. Hybrid Switching Diffusions: Properties and Applications. Springer, 2010.
