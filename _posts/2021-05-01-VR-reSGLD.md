---
title: 'Acceleration via Replica Exchange and Variance Reduction'
date: 2021-05-01
permalink: /posts/replica/
tags:
  - replica exchange
  - parallel tempering
  - simulations of multi-modal distributions
  - variance reduction
---

Despite the popularity for the gradient-based variance reduction [1,2], a discrepancy between theory and practice is whether we should avoid the gradient noise in non-convex problems. To fill in the gap, we only focus on the energy variance reduction to exploit exponential accelerations but no longer consider the gradient variance reduction.

<p float="left" align="center">
  <img src="/images/VR-reSGLD/SGLD.gif" width="185" title="SGLD"/>
  <img src="/images/VR-reSGLD/reSGLD_vs_VR_reSGLD.gif" width="340" alt="Made with Angular" title="reSGLD vs VR-reSGLD" />
</p>

Coming soon

## References:

1. Dubey, Reddi, Poczos, Smola, Xing, Williamson. Variance Reduction in Stochastic Gradient Langevin Dynamics. NeurIPS'16.
2. Xu, Chen, Zou, Gu. Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization. NeurIPS'18.
3. Chen, Chen, Dong, Peng, Wang. Accelerating Nonconvex Learning via Replica Exchange Langevin Diffusion. ICLR'19.
4. Deng, Feng, Gao, Liang, Lin. Non-Convex Learning via Replica Exchange Stochastic Gradient MCMC. ICML'20.
5. Deng, Feng, Karagiannis, Lin, Liang. Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction. ICLR'21.
6. George Yin and Chao Zhu. Hybrid Switching Diffusions: Properties and Applications. Springer, 2010.
