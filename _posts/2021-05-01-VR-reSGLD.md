---
title: 'Acceleration via Replica Exchange and Variance Reduction'
date: 2021-05-01
permalink: /posts/replica/
tags:
  - replica exchange
  - parallel tempering
  - simulations of multi-modal distributions
  - variance reduction
---

Despite the popularity for the gradient-based variance reduction [1,2], a discrepancy between theory and practice is whether we should avoid the gradient noise in non-convex problems. To fill in the gap, we only focus on the energy variance reduction to exploit exponential accelerations but no longer consider the gradient variance reduction.

A standard sampling algorithm is the stochastic gradient Langevin dynamics, which is a scalable numerical discretization of a stochastic differential equation (SDE) as follows:

$\beta_{k+1}=\beta_k - \eta \nabla U(\beta_k) + \sqrt{2\tau_1} \xi_k$,

where $\beta\in\mathbb{R}^d$ $U(\cdot)$ is a energy function and $\xi$ is a d-dimensional Gaussian vector.


Simulated annealing is adopted in almost every espect in deep learning, which proposes to anneal temperatures to concentrate the probability measures towards the global optima. Such a strategy, however, fails in uncertainty estimations for reliable predictions. To accelerate the simulations, replica exchange proposes to run multiple stochastic processes with different temperatures, where interactions between different SGLD chains are conducted in a manner that encourages large jumps.

<p align="center">
    <img src="/images/reSGLD_exploitation_exploration.png" width="500" height="250" />
</p>

By allowing the two particles to swap, the positions are likely to change from $(\beta^{(1)}, \beta^{(2)})$ 
(1)
t
, β
(2)
t
) to (β
(2)
t+dt, β
(1)
t+dt) with a
swapping rate r(1 ∧ S(β
(1)
t
, β
(2)
t
))dt, where the constant
r ≥ 0 is the swapping intensity, and S(·, ·) satisfies




<p float="left" align="center">
  <img src="/images/VR-reSGLD/SGLD.gif" width="185" title="SGLD"/>
  <img src="/images/VR-reSGLD/reSGLD_vs_VR_reSGLD.gif" width="340" alt="Made with Angular" title="reSGLD vs VR-reSGLD" />
</p>

Coming soon

## References:

1. Dubey, Reddi, Poczos, Smola, Xing, Williamson. Variance Reduction in Stochastic Gradient Langevin Dynamics. NeurIPS'16.
2. Xu, Chen, Zou, Gu. Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization. NeurIPS'18.
3. Chen, Chen, Dong, Peng, Wang. Accelerating Nonconvex Learning via Replica Exchange Langevin Diffusion. ICLR'19.
4. Deng, Feng, Gao, Liang, Lin. Non-Convex Learning via Replica Exchange Stochastic Gradient MCMC. ICML'20.
5. Deng, Feng, Karagiannis, Lin, Liang. Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction. ICLR'21.
6. George Yin and Chao Zhu. Hybrid Switching Diffusions: Properties and Applications. Springer, 2010.
