---
title: 'Replica Exchange Stochastic Gradient Langevin Dynamics and Beyond'
date: 2020-08-15
permalink: /posts/2012/08/blog-post-1/
tags:
  - replica exchange
  - parallel tempering
  - stochastic gradient Langevin dynamics
  - variance reduction
  - Growall inequality
  - generalized Girsanov theorem
  - change of Poisson measure
  - Markov jump process
---


Replica exchange, also known as parallel tempering, is an important technique for accelerating the convergence of the conventional Markov Chain Monte Carlo
algorithms. However, such a method requires the evaluation of the energy function based on the full dataset and is not scalable to big data. The na¨ıve implementation of reMC in mini-batch settings introduces large biases, which cannot be directly extended to the stochastic gradient MCMC, the standard sampling method for simulating from deep neural networks. We propose an adaptive replica exchange SGMCMC (reSGMCMC) to automatically correct the bias and study the corresponding properties. The analysis implies an acceleration accuracy trade-off in the numerical discretization of a Markov jump process in a stochastic environment. 

Replica exchange stochastic gradient Langevin dynamics has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential acceleration for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\mathcal{W}_2$) distance. 


