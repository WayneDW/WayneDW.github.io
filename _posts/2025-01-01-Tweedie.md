---
title: 'Tweedie Formula'
subtitle: A simple empirical Bayesian posterior mean
date: 2025-01-01
permalink: /posts/tweedie_formula/
category: Theory
---

Tweedie’s formula {% cite Tweedie %} is a key method in empirical Bayes to obtain the posterior mean based on observed data. Assume we have a data $\mathrm{X_0}$ and a noisy measurement $\mathrm{X_1}$ which follow

$$\begin{align}
    \mathrm{X_1 = \alpha X_0 + \sigma Z},\label{transform}
\end{align}$$

where $\mathrm{X_0 \sim p_0}$, $\mathrm{X_1 \sim p_1}$, and $\mathrm{Z \sim N(0, {I})}$.


We next study the unconditional score function $\mathrm{\log p_1(x_1)}$:

$$\begin{align}
    \mathrm{\nabla_{x_1} \log p_1(x_1)} &= \mathrm{\frac{1}{p_1(x_1)} \nabla_{x_1} p_1(x_1)} \notag \\
                           &= \mathrm{\frac{1}{p_1(x_1)} \nabla_{x_1} \int p_{1|0}(x_1|x_0) p_0(x_0) d x_0} \notag \\
                           &= \mathrm{\frac{1}{p_1(x_1)} \int p_{1|0}(x_1|x_0) \nabla_{x_1} \log p_{1|0}(x_1|x_0) p_0(x_0) d x_0} \notag \\
                           &= \mathrm{\int p_{0|1}(x_0|x_1) \nabla_{x_1} \log p_{1|0}(x_1|x_0) d x_0} \notag \\
                           &= \mathrm{\int p_{0|1}(x_0|x_1) \frac{\alpha x_0-x_1}{\sigma^2} d x_0} \notag \\
                           &= \mathrm{\frac{\alpha E[x_0|x_1] - x_1}{\sigma^2}} \notag \\
\end{align}$$

The posterior mean $\mathrm{E[x_0\|x_1]}$ behaves like a denoiser, as described by {% cite chung2023diffusion %} 

$$\begin{align}
    \mathrm{E[x_0\\|x_1]=\frac{1}{\alpha}\big(x_1+\sigma^2 \nabla_{x_1} \log p_1(x_1)}\big).\label{tweedie}
\end{align}$$


### Connections to Diffusion Models

Consider a simple OU process {% cite score_sde %} {% cite ho2020denoising %}

$$\begin{align}
    \mathrm{d X_t = -\frac{\beta_t}{2} X_t dt + \sqrt{\beta_t} d W_t,}\label{vpsde}
\end{align}$$

where $t\in[0, 1]$. $\mathrm{X_0 \sim p_0}$ and $\mathrm{X_1 \sim p_1}$. With the help of integration factors, the above process yields a simple closed-form solution similar to Eq.\eqref{transform}

$$\begin{align}
    \mathrm{X_1 = \alpha X_0 + \sigma Z},\notag
\end{align}$$

where $\mathrm{\alpha=e^{-\frac{1}{2}\int_0^1 \beta_s d s}}$, $\mathrm{\sigma=\sqrt{1-e^{-\int_0^1 \beta_s d s}}}$.

Additionally, the reverse of Eq.\eqref{vpsde} {% cite anderson1982reverse %} follows that 

$$\begin{align}
    \mathrm{d X_t = -\frac{\beta_t}{2} X_t dt -\beta_t \nabla \log p_t(X_t)dt + \sqrt{\beta_t} d W_t}.\label{bsde}
\end{align}$$

The above provides an iterative process to generate $\mathrm{X_0}$ using multiple scores $$\mathrm{\{\nabla \log p_t(X_t)\}_{t=0}^1}$$. In contrast, Tweedie’s formula in Eq.\eqref{tweedie} requires only one score function, $\mathrm{\nabla_{x_1} \log p_1(x_1)}$, to generate $\mathrm{X_0}$. 

### Summary

Does this imply that the iterative process in Eq.\eqref{bsde} is not necessary? This might be the case if we can obtain a sufficiently accurate score estimator, $\mathrm{s_{\theta} \approx \nabla_{x_1} \log p_1(x_1)}$, given a large enough dataset $\mathrm{x_0\sim p_0}$. However, this is not trivial for highly complex, high-dimensional, multimodal real-world data.