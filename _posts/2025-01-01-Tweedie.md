---
title: 'Tweedie Formula and Pitfalls'
subtitle: A simple empirical Bayesian posterior mean
date: 2025-01-01
permalink: /posts/tweedie_formula/
category: Theory
---

Tweedie Formula

Assume we have a data distribution $\mathrm{X}$ and a noisy measurement $\mathrm{Y}$ which follow

$$\begin{align}
    \mathrm{Y = \alpha X + \sigma Z},\notag
\end{align}$$

where $\mathrm{Z \sim N(0, {I})}$.


We next study the score function $\mathrm{\log p_Y(y)}$:

$$\begin{align}
    \mathrm{\nabla_{Y} \log p_Y(y)} &= \mathrm{\frac{1}{p_Y(y)} \nabla_{Y} p_Y(y)} \notag \\
                           &= \mathrm{\frac{1}{p_Y(y)} \nabla_{Y} \int p_Y(y|x) p(x) d x} \notag \\
                           &= \mathrm{\frac{1}{p_Y(y)} \int p_Y(y|x) \nabla_{Y} \log p_Y(y|x) p(x) d x} \notag \\
                           &= \mathrm{\int p_Y(x|y) \nabla_{Y} \log p_Y(y|x) d x} \notag \\
                           &= \mathrm{\int p_Y(x|y) \frac{\alpha x-y}{\sigma^2} d x} \notag \\
                           &= \mathrm{\frac{\alpha E[x|y] - y}{\sigma^2}} \notag \\
\end{align}$$

In other words, the posterior mean $\mathrm{E[x\|y]}$ [cite Tweedie: Efron, Bradley. 2011. “Tweedie’s Formula and Selection Bias.” Journal of the American Statistical Association 106 (496): 1602–14.] follows that 


$$\begin{align}
    \mathrm{E[x\\|y]=\frac{1}{\alpha}\big(y+\sigma^2 \nabla_{Y} \log p_Y(y)}\big).\label{tweedie}
\end{align}$$


### Connections to Diffusion Models

Eq.\eqref{tweedie} provides a one-step generation formula for generative modeling tasks cite [DDPM]. In contrast, the well-known DDPM requires an iterative process with multiple score estimators.