---
title: 'Couplings and Monte Carlo Methods'
date: 2021-08-01
permalink: /posts/Couplings/
tags:
  - Synchronous Coupling
  - Maximal Coupling
  - Reflection Coupling
---


TBD

The synchronous coupling approach can be used to prove the convergence of stochastic gradient Langevin dynamics for strongly log-concave distributions.

Let $\theta_k\in \mathbb{R}^d$ be the $k$-th iterate of the following stochastic gradient Langevin algorithm.
\begin{align}\label{eq:sgld}
    \theta_{k+1}=\theta_k -\eta \nabla \widetilde f(\theta_k)+\sqrt{2\tau\eta}\xi_k,
\end{align}
where $\eta$ is the learning rate, $\tau$ is the temperature, $\xi_k$ is a standard $d$-dimensional Gaussian vector, and $\nabla \widetilde f(\theta)$ is an unbiased estimate of the exact gradient $\nabla f(\theta)$.




## Assumptions

**Smoothness** We say $f$ is $L$-smooth if for some $L>0$
\begin{align}\label{def:strong_convex}
f(y)\leq f(x)+\langle \nabla f(x),y-x \rangle+\frac{L}{2}|| y-x ||^2_2\quad \forall x, y\in \mathbb{R}^d.
\end{align}


**Strong convexity**
We say $f$ is $m$-strongly convex if for some $m>0$
\begin{align}\label{def:smooth}
f(x)\geq f(y)+\langle \nabla f(y),x-y \rangle + \frac{m}{2} || y-x ||_2^2\quad \forall x, y\in \mathbb{R}^d.
\end{align}


**Bounded variance** The variance of stochastic gradient $\nabla \widetilde f(x)$ in each device is upper bounded such that
\begin{align}\label{def:variance}
\mathbb{E}[||\nabla \widetilde f(x)-\nabla f(x)||^2] \leq \sigma^2 d,\quad \forall x\in \mathbb{R}^d.
\end{align}




## A Crude Result/ Theorem

Assume assumptions \ref{def:strong_convex}, \ref{def:smooth}, and \ref{def:variance} hold. For any learning rate $\eta \in (0 , \min {1, {m}/{L^2} })$  and $| \theta_0-\theta_* | \leq \sqrt{d} {D}$, where $\theta_*$ is a stationary point. Then


\begin{align}
W_2(\mu_k, \pi) \leq e^{-{mk\eta}/{2}} \cdot 2 ( \sqrt{d} {D} + \sqrt{d/m} ) + \sqrt{ 2d (\sigma^2+L^2 G\eta) / m^2},
\end{align}

where $\mu_k$ denotes the probability measure of $\theta_k$ and $G:=25(\tau+m\mathcal{D}^2+\sigma^2)$.


## Proof
Denote $\theta_t$ as the continuous-time interpolation of the stochastic gradient Langevin dynamics as follows

\begin{align}\label{eq:continuous_interpolation}
d {\theta}_t = - \nabla \widetilde f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) d t + \sqrt{2\tau} d W_t,
\end{align}

where ${\theta}_0=\theta_0$. For any $k\in \mathbb{N}^{+}$ and a time $t$ that satisfies $t=k\eta$, it is apparent that $\widehat\mu_t=\mathcal{L}({\theta}_t)$ is the same as $\mu_k=\mathcal{L}(\theta_k)$, where $\mathcal{L}(\cdot)$ denotes a distribution of a random variable. In addition, we define an auxiliary process $(\theta^*_t)$ that starts from the stationary distribution $\pi$

\begin{align}
d \theta^*_t = - \nabla f(\theta^*_t) d t + \sqrt{2\tau} d W^*_t.
\end{align}



Consider It\^{o}'s formula for the sequence of $\frac{1}{2}  \| \theta_t - \theta^*_t \|_2^2$
\begin{align}
&\frac{1}{2} \d  \| \theta_t - \theta^*_t \|_2^2 \\
&= \lrw{ \theta_t - \theta^*_t, \d \theta_t - \d \theta^*_t } + \mathrm{Tr}[ \d^2 \theta_t - \d^2 \theta^*_t ] \\
&= \lrw{ \theta_t - \theta^*_t, \big(\nabla f(\theta^*_t) -\nabla\widetilde  f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \big) \d t + \sqrt{2\tau}\big( \d \hat{W}_t - \d W^*_t \big) } + 2\tau \mathrm{Tr}[ \d^2 \hat{W}_t - \d^2 W^*_t ].
\end{align}



Taking $\hat{W}_t = W^*_t$ defines a coupling between the two processes and leads to
\begin{align}
\frac{1}{2} \d \| \theta_t - \theta^*_t \|_2^2
&= \lrw{ \theta_t - \theta^*_t, \nabla f(\theta^*_t)-\nabla f(\theta_t)} \d t+ \lrw{ \theta_t - \theta^*_t,  \nabla f(\theta_{t}) - \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor})  } \d t \\
&\qquad \qquad+ \lrw{ \theta_t - \theta^*_t, \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) - \nabla \widetilde f(\widehat \theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) } \d t \\
&\leq - m \| \theta_t - \theta_t^* \|_2^2 \d t + \frac{m}{4} \| \theta_t - \theta^*_t \|_2^2 \d t + \frac{1}{m} \big\| \nabla f(\theta_{t}) - \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \big\|_2^2 \d t \\
&\qquad\qquad  + \frac{m}{4} \| \theta_t - \theta^*_t \|_2^2\d t + \frac{1}{m} \lrn{\nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) - \nabla \widetilde f(\widehat \theta_{\eta\lfloor\frac{t}{\eta} \rfloor})}_2^2\d t\\
&\leq  - \frac{m}{2} \| \theta_t - \theta_t^* \|_2^2 \d t + \frac{d\sigma^2}{m}\d t +\frac{1}{m} \big\| \nabla f(\theta_{t}) - \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \big\|_2^2 \d t\\
&\leq - \frac{m}{2} \| \theta_t - \theta_t^* \|_2^2 \d t +\frac{d\sigma^2}{m}\d t+ \frac{L^2}{m} \big\| \theta_{t} - \theta_{\eta\lfloor\frac{t}{\eta} \rfloor} \big\|_2^2 \d t,
\end{align}
where the first inequality follows from the strong-convexity property and $ab\leq  (\frac{\sqrt{m}}{2}a)^2+({\frac{1}{\sqrt{m}}}b)^2$; in particular, we don't attempt to optimize the constants of $-\frac{m}{2}$ for the item $\| \theta_t - \theta_t^* \|_2^2$; the second and third inequalities follow by the smoothness assumptions \ref{def:variance} and \ref{def:smooth}, respectively.


Now apply Gr\"{o}nwall's inequality to the preceding inequality and take expectation respect to a coupling $(\theta_t, \theta^*_t) \sim \Gamma(\widehat\mu_t,\pi)$
\begin{align}\label{eq:1st_gronwall}
     \E{ \|\theta_t - \theta^*_t \|_2^2}\leq  \E{\| \theta_0 - \theta^*_0 \|_2^2} e^{-mt}+\frac{2}{m}\int_0^t \bigg(d\sigma^2+ L^2\underbrace{\E{ \big\| \theta_{s} - \theta_{\eta\lfloor\frac{s}{\eta} \rfloor} \big\|_2^2}}_{\mathcal{I}} \bigg) e^{-(t-s)m} \d s. 
\end{align}




%\paragraph{Estimate of $\mathcal{I}$} 

%\paragraph{Proof of Theorem \ref{thm:non_asymptotic} (continued)} 
Plugging the estimate of $\mathcal{I}$ in Lemma~\ref{lem:estimate_of_I} %Eq.~\eqref{eq:combined_bound} 
into Eq.~\eqref{eq:1st_gronwall}, we have
\begin{align}
    \E{ \| \theta_t - \theta^*_t \|_2^2}&\leq  \E{\| \theta_0 - \theta^*_0 \|_2^2} e^{-mt}+\frac{2d}{m} (\sigma^2+L^2 G\eta) \int_0^t  e^{-(t-s)m} \d s\\
     &\leq \E{\| \theta_0 - \theta^*_0 \|_2^2} e^{-mt}+\frac{2d}{m^2} (\sigma^2+L^2 G\eta).
\end{align}

Recall that $\theta_k$ and $\widehat\theta_{t\eta}$ have the same distribution $\mu_k$. 


By the definition of $W_2$ distance, we have
\begin{align}
W_2(\mu_k, \pi) 
%\leq \left(\E{ \| \theta_{k\eta} - \theta^*_{k\eta} \|_2^2}\right)^{1/2}
\leq & ~ e^{-{mk\eta}/{2}} \cdot W_2(\mu_0, \pi) + \sqrt{ 2d (\sigma^2+L^2 G\eta) / m^2} \\
\leq & ~ e^{-{mk\eta}/{2}} \cdot 2 (\| \theta_0 - \theta_* \|_2 +  \sqrt{d/m} )+ \sqrt{ 2d(\sigma^2+L^2 G\eta) / m^2} \\
\leq & ~ e^{-{mk\eta}/{2}} \cdot 2 ( \sqrt{d} {\cal D} +  \sqrt{d/m} )+  \sqrt{ 2 d(\sigma^2+L^2 G\eta) / m^2},
\end{align}
where the first inequality follows by applying $(a+b)^{1/2}\leq |a|^{1/2}+|b|^{1/2}$, the second one follows by Lemma \ref{lem:W2_init_bound}, and the last step follows from assumption on $\| \theta_0 - \theta_* \|_2$.




