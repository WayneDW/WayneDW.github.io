---
title: 'Couplings and Monte Carlo Methods'
date: 2021-08-01
permalink: /posts/Couplings/
tags:
  - Synchronous Coupling
  - Maximal Coupling
  - Reflection Coupling
---


In this blog, I apply a synchronous coupling approach to prove the convergence of stochastic gradient Langevin dynamics for log-concave distributions.

Let $\theta_k\in \mathbb{R}^d$ be the $k$-th iterate of the following stochastic gradient Langevin algorithm.
\begin{align}\label{eq:sgld}
    \theta_{k+1}=\theta_k -\eta \nabla \widetilde f(\theta_k)+\sqrt{2\tau\eta}\xi_k,
\end{align}
where $\eta$ is the learning rate, $\tau$ is the temperature, $\xi_k$ is a standard $d$-dimensional Gaussian vector, and $\nabla \widetilde f(\theta)$ is an unbiased estimate of the exact gradient $\nabla f(\theta)$.






Smoothness We say $f$ is $L$-smooth if for some $L>0$
\begin{align*}
f(y)\leq f(x)+\langle \nabla f(x),y-x \rangle+\frac{L}{2}\| y-x \|^2_2\quad \forall x, y\in \R^d.
\end{align*}


Strong convexity
We say $f$ is $m$-strongly convex if for some $m>0$
\begin{align*}
f(x)\geq f(y)+\langle \nabla f(y),x-y \rangle + \frac{m}{2} \| y-x \|_2^2\quad \forall x, y\in \R^d.
\end{align*}


Bounded variance
The variance of stochastic gradient $\nabla \widetilde f(x)$ in each device is upper bounded such that
\begin{align*}
\mathbb{E}[\|\nabla \widetilde f(x)-\nabla f(x)\|^2] \leq \sigma^2 d,\quad \forall x\in \R^d.
\end{align*}





















Assume assumptions \ref{def:strong_convex}, \ref{def:smooth}, and \ref{def:variance} hold. For any learning rate $\eta \in (0 , \min\{1, {m}/{L^2}\})$ and $\| \theta_0-\theta_* \|_2 \leq \sqrt{d} \mathcal{D}$, where $\theta_{\star}$ is a stationary point. Then
\begin{align*}
W_2(\mu_k, \pi) \leq e^{-{mk\eta}/{2}} \cdot 2 ( \sqrt{d} {\cal D} + \sqrt{d/m} ) + \sqrt{ 2d (\sigma^2+L^2 G\eta) / m^2}, %\frac{L}{m} \sqrt{G\eta d},
\end{align*}
where $\mu_k$ denotes the probability measure of $\theta_k$ and $G:=25(\tau+m\mathcal{D}^2+\sigma^2)$.
\end{theorem}


\begin{proof}
Denote $\htheta_t$ as the continuous-time interpolation of the stochastic gradient Langevin dynamics as follows
\begin{align}\label{eq:continuous_interpolation}
\d {\htheta}_t = - \nabla \widetilde f(\htheta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \d t + \sqrt{2\tau} \d \hat{W}_t,
\end{align}
where ${\htheta}_0=\theta_0$. For any $k\in \mathbb{N}^{+}$ and a time $t$ that satisfies $t=k\eta$, it is apparent that $\widehat\mu_t=\mathcal{L}({\htheta}_t)$ is the same as $\mu_k=\mathcal{L}(\theta_k)$, where $\mathcal{L}(\cdot)$ denotes a distribution of a random variable. In addition, we define an auxiliary process $(\theta^*_t)$ that starts from the stationary distribution $\pi$
\begin{align}
\d \theta^*_t = - \nabla f(\theta^*_t) \d t + \sqrt{2\tau} \d W^*_t.
\end{align}
Consider It\^{o}'s formula for the sequence of $\frac{1}{2}  \| \htheta_t - \theta^*_t \|_2^2$
\begin{align*}
&\frac{1}{2} \d  \| \htheta_t - \theta^*_t \|_2^2 \\
&= \lrw{ \htheta_t - \theta^*_t, \d \htheta_t - \d \theta^*_t } + \mathrm{Tr}[ \d^2 \htheta_t - \d^2 \theta^*_t ] \\
&= \lrw{ \htheta_t - \theta^*_t, \big(\nabla f(\theta^*_t) -\nabla\widetilde  f(\htheta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \big) \d t + \sqrt{2\tau}\big( \d \hat{W}_t - \d W^*_t \big) } + 2\tau \mathrm{Tr}[ \d^2 \hat{W}_t - \d^2 W^*_t ].
\end{align*}
Taking $\hat{W}_t = W^*_t$ defines a coupling between the two processes and leads to
\begin{align*}
\frac{1}{2} \d \| \htheta_t - \theta^*_t \|_2^2
&= \lrw{ \htheta_t - \theta^*_t, \nabla f(\theta^*_t)-\nabla f(\htheta_t)} \d t+ \lrw{ \htheta_t - \theta^*_t,  \nabla f(\htheta_{t}) - \nabla f(\htheta_{\eta\lfloor\frac{t}{\eta} \rfloor})  } \d t \\
&\qquad \qquad+ \lrw{ \htheta_t - \theta^*_t, \nabla f(\htheta_{\eta\lfloor\frac{t}{\eta} \rfloor}) - \nabla \widetilde f(\widehat \theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) } \d t \\
&\leq - m \| \htheta_t - \theta_t^* \|_2^2 \d t + \frac{m}{4} \| \htheta_t - \theta^*_t \|_2^2 \d t + \frac{1}{m} \big\| \nabla f(\htheta_{t}) - \nabla f(\htheta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \big\|_2^2 \d t \\
&\qquad\qquad  + \frac{m}{4} \| \htheta_t - \theta^*_t \|_2^2\d t + \frac{1}{m} \lrn{\nabla f(\htheta_{\eta\lfloor\frac{t}{\eta} \rfloor}) - \nabla \widetilde f(\widehat \theta_{\eta\lfloor\frac{t}{\eta} \rfloor})}_2^2\d t\\
&\leq  - \frac{m}{2} \| \htheta_t - \theta_t^* \|_2^2 \d t + \frac{d\sigma^2}{m}\d t +\frac{1}{m} \big\| \nabla f(\htheta_{t}) - \nabla f(\htheta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \big\|_2^2 \d t\\
&\leq - \frac{m}{2} \| \htheta_t - \theta_t^* \|_2^2 \d t +\frac{d\sigma^2}{m}\d t+ \frac{L^2}{m} \big\| \htheta_{t} - \htheta_{\eta\lfloor\frac{t}{\eta} \rfloor} \big\|_2^2 \d t,
\end{align*}
where the first inequality follows from the strong-convexity property and $ab\leq  (\frac{\sqrt{m}}{2}a)^2+({\frac{1}{\sqrt{m}}}b)^2$; in particular, we don't attempt to optimize the constants of $-\frac{m}{2}$ for the item $\| \htheta_t - \theta_t^* \|_2^2$; the second and third inequalities follow by the smoothness assumptions \ref{def:variance} and \ref{def:smooth}, respectively.


Now apply Gr\"{o}nwall's inequality to the preceding inequality and take expectation respect to a coupling $(\htheta_t, \theta^*_t) \sim \Gamma(\widehat\mu_t,\pi)$
\begin{align}\label{eq:1st_gronwall}
     \E{ \|\htheta_t - \theta^*_t \|_2^2}\leq  \E{\| \htheta_0 - \theta^*_0 \|_2^2} e^{-mt}+\frac{2}{m}\int_0^t \bigg(d\sigma^2+ L^2\underbrace{\E{ \big\| \htheta_{s} - \htheta_{\eta\lfloor\frac{s}{\eta} \rfloor} \big\|_2^2}}_{\mathcal{I}} \bigg) e^{-(t-s)m} \d s. 
\end{align}
% \Zhao{In the second term of the above equation, is that $t/\eta$ actually $s/\eta$?} \textcolor{red}{nice catch}

%\paragraph{Estimate of $\mathcal{I}$} 

%\paragraph{Proof of Theorem \ref{thm:non_asymptotic} (continued)} 
Plugging the estimate of $\mathcal{I}$ in Lemma~\ref{lem:estimate_of_I} %Eq.~\eqref{eq:combined_bound} 
into Eq.~\eqref{eq:1st_gronwall}, we have
\begin{align*}
    \E{ \| \htheta_t - \theta^*_t \|_2^2}&\leq  \E{\| \htheta_0 - \theta^*_0 \|_2^2} e^{-mt}+\frac{2d}{m} (\sigma^2+L^2 G\eta) \int_0^t  e^{-(t-s)m} \d s\\
     &\leq \E{\| \htheta_0 - \theta^*_0 \|_2^2} e^{-mt}+\frac{2d}{m^2} (\sigma^2+L^2 G\eta).
\end{align*}

Recall that $\theta_k$ and $\widehat\theta_{t\eta}$ have the same distribution $\mu_k$. 


By the definition of $W_2$ distance, we have
\begin{align*}
W_2(\mu_k, \pi) 
%\leq \left(\E{ \| \htheta_{k\eta} - \theta^*_{k\eta} \|_2^2}\right)^{1/2}
\leq & ~ e^{-{mk\eta}/{2}} \cdot W_2(\mu_0, \pi) + \sqrt{ 2d (\sigma^2+L^2 G\eta) / m^2} \\
\leq & ~ e^{-{mk\eta}/{2}} \cdot 2 (\| \theta_0 - \theta_* \|_2 +  \sqrt{d/m} )+ \sqrt{ 2d(\sigma^2+L^2 G\eta) / m^2} \\
\leq & ~ e^{-{mk\eta}/{2}} \cdot 2 ( \sqrt{d} {\cal D} +  \sqrt{d/m} )+  \sqrt{ 2 d(\sigma^2+L^2 G\eta) / m^2},
\end{align*}
where the first inequality follows by applying $(a+b)^{1/2}\leq |a|^{1/2}+|b|^{1/2}$, the second one follows by Lemma \ref{lem:W2_init_bound}, and the last step follows from assumption on $\| \theta_0 - \theta_* \|_2$.



\end{proof}

