---
title: 'Sequential Monte Carlo'
subtitle: A principled framework for nonlinear state-space models
date: 2025-08-06
permalink: /posts/sequential_monte_carlo/
category: State Space Model
---

Sequential Monte Carlo (SMC), also known as particle filtering, is a flexible, simulation-based sampling method for computing the posterior distribution of nonlinear state-space models.

<!-- The target distribution given latent variables $$\mathrm{x_{1:t}=(x_1, \cdots, x_t)}$$ follows

$$\begin{align}
    \mathrm{\gamma_t(x_{1:t}):=\frac{1}{Z_t} \widetilde \gamma_t(x_{1:t}), \ \  t\in\{1,2,\cdots, T\}},\notag
\end{align}$$ -->


Suppose we have a nonlinear state space model

$$\begin{align}
    \mathrm{x_t|x_{t-1}}&\sim \mathrm{p(\cdot|x_{t-1}, \theta)},\notag \\
    \mathrm{y_t|x_t}&\sim \mathrm{p(\cdot|x_t, \theta)},\notag
\end{align}$$

where $\mathrm{\theta}$ denotes the hyperparameter of the dynamics and is subject to estimation.





The marginal likelihood of the model follows that

$$\begin{align}
\mathrm{p(\mathbf{y}_{1:T}|\theta)=\prod_{t=1}^T p(\mathbf{y}_{t}|\mathbf{y}_{1:t-1},\theta)},\label{marginal_pdf}
\end{align}$$

where we fix $$\mathrm{p(\mathbf{y}_{1}\\|\mathbf{y}_{1:0},\theta)=p(\theta)}$$.

<!-- #|\mathbf{y}_{1:0},\theta)=p(\theta)$ -->

$$\begin{align}
\mathrm{p(\mathbf{y}_{t}|\mathbf{y}_{1:t-1},\theta)}&=\mathrm{\int p(\mathbf{y}_{t}, \mathbf{x}_{t}|\mathbf{y}_{1:t-1},\theta) d \mathbf{x}_{t}},\notag \\
                                                    &=\mathrm{\int p(\mathbf{y}_{t} | \mathbf{x}_{t},\theta) p(\mathbf{x}_{t} | \mathbf{y}_{1:t-1},\theta)d \mathbf{x}_{t}},\notag \\
                                                    &=\mathrm{\lim_{N\rightarrow \infty}\sum_{i=1}^N p(\mathbf{y}^{(i)}_{t} | \mathbf{x}^{(i)}_{t},\theta) w_{t-1}^{(i)}}.\notag \\
\end{align}$$

$$\begin{align}
\mathrm{w_{t-1}=p(\mathbf{x}_{t} | \mathbf{y}_{1:t-1},\theta)}&=\mathrm{\int p(x_t|x_{t-1}, \theta) p(x_{t-1}|y_{1:t-1}, \theta) d x_{t-1}}\notag \\
                                                              &=\mathrm{\int p(x_t|x_{t-1}, \theta) \frac{p(y_{t-1}|x_{t-1}, \theta) p(x_{t-1}|y_{1:t-2}, \theta)}{p(y_{t-1}|y_{1:t-2}, \theta)} d x_{t-1}}\notag.
\end{align}$$


The joint PDF follows

$$\begin{align}
\mathrm{p(\mathbf{x}, \mathbf{y})=p(x_1)g(y_1|x_1) \prod_{t=2}^T f(x_t|x_{t-1}) g(y_t|x_t)},\label{joint_pdf}
\end{align}$$



<!-- where the final normalized density follows $$\mathrm{\gamma_T(x_{1:T})=p(\mathbf{x}, \mathbf{y})}$$.  -->


#### Sequential Importance Sampling

We can consider a proposal 

$$\begin{align}
    \mathrm{q_t(x_{1:t})=q_{t-1}(x_{1:t-1}) q_t(x_t|x_{1:t-1})},\notag
\end{align}$$

The importance weights follow that 

$$\begin{align}
    \mathrm{\widetilde w_t(x_{1:t})}&=\mathrm{\frac{\widetilde \gamma_t(x_{1:t})}{q_t(x_{1:t})}=\frac{\widetilde \gamma_{t-1}(x_{1:t-1})}{q_{t-1}(x_{1:t-1})}\frac{\widetilde \gamma_t(x_t|x_{1:t-1})}{q_t(x_t|x_{1:t-1})}},\notag\\
\end{align}$$

It suffers from weight degeneracy issues. 

### Sequential Monte Carlo

Simulate a proposal 

$$\begin{align}
    \mathrm{q_t(x_{1:t})=\widehat \gamma_{t-1}(x_{1:t-1}) q_t(x_t|x_{1:t-1})},\notag
\end{align}$$


<!-- ### Learning Proposals and Twisting Targets

The optimal proposal 

$$\begin{align}
    \mathrm{q^{\star}_t(x_t\\|x_{1:t-1})=\gamma_t(x_t|x_{1:t-1})},\notag
\end{align}$$

By Eq.\eqref{joint_pdf}, the proposal in example 1.2.1 in {% cite elements_smc %} follows that 

$$\begin{align}
    \mathrm{q^{\star}_t(x_t\\|x_{1:t-1})=\frac{\gamma_t(x_t)}{\gamma_t(x_{1:t-1})}=f(x_t|x_{t-1}) g(y_t|x_t)},\notag
\end{align}$$


### Twisted SMC methods: Adapting the Target Distribution


The goal is to see if we can simulate the optimal proposal in this way

$$\begin{align}
    \mathrm{q^{\star}_t(x_t\\|x_{1:t-1})=\gamma^{\star}_t(x_t\\|x_{1:t-1})=\gamma_T(x_t\\|x_{1:t-1})=p(x_t|x_{1:t-1}, y_{1:T})},\notag
\end{align}$$

The propose is optimized in a global sense. -->

