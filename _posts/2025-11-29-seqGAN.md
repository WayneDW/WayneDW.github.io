---
title: 'Sequence Generative Adversarial Nets (SeqGAN)'
subtitle: Adversarial generations of discrete sequences 
date: 2025-11-29
permalink: /posts/seqGAN/
category: Sampling
---

Generative Adversarial Networks (GANs) {% cite goodfellow2014generative %} were once among the best generative models for image data. Their adversarial training enables the generator ($$\mathbf{G}$$) to produce high-fidelity samples that even a strong discriminator ($$\mathbf{D}$$) cannot distinguish. $$\mathbf{D}$$ and $$\mathbf{G}$$ play the minimax game:

$$
\begin{align}
& \mathrm{\min_{G} \max_{D} \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)]
+ \mathbb{E}_{z\sim G} [\log(1 - D(z))]}. \notag
\end{align}
$$

 Despite this success, however, its generation of **discrete sequences** remains poorly understood due to:

1. The loss of $$\mathbf{D}$$ from $$\mathbf{G}$$ samples is not **non-differentiable** on discrete data;
2. $$\mathbf{D}$$ cannot evaluate a **intermediate** state $$\mathrm{z_{1:t}}$$ unless $$\mathrm{t}$$ reaches the full sequence length $$\mathrm{T}$$. 


### Sequence Generative Adversarial Nets (SeqGAN)


SeqGAN resolves these issues by modeling the generator $$\mathbf{G}$$ as a policy for sequence generation and using the discriminator $$\mathbf{D}$$ to provide the reward signal.

1. to tackle the **non-differentiability**, policy gradient methods {% cite williams1992simple %} was proposed to shift from the gradient of discrete actions to the probability space using the log-derivative trick:

$$
\begin{align}
& \nabla \mathrm{\mathbb{E}_{\mathbf{z}\sim G_{\theta}}[D(z)]=\sum_z D(z) \nabla_{\theta}G_{\theta}(z)= \sum_z D(z) G_{\theta}(z) \nabla_{\theta}\log G_{\theta}(z)=\mathbb{E}[D(z) \nabla_{\theta} \log G_{\theta}(z)]}. \notag
\end{align}
$$

<!-- where $$\mathrm{D}$$, where $$\mathrm{t\in \{1, 2, \cdots, T\}}$$, is the intermediate reward signal to be defined later. -->

2. for the intermediate state $$\mathrm{z_{1:t}}$$, we study Monte Carlo search with a roll-out policy 

<figure style="text-align: center;">
    <img src="/images/seqGAN.png" width="550" height="200" />
    <figcaption> SeqGAN: $\mathbf{G}$ acts as a sequence-generation policy; $\mathbf{D}$ provides the rewards {% cite yu2017seqgan %}. </figcaption>
</figure>




$$
\begin{align}
&  \notag
\end{align}
$$


<!-- On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes

One-step Diffusion with Distribution Matching Distillation 
https://arxiv.org/pdf/2311.18828 -->

<!-- SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient -->

### On-policy Distillation

<br><br><br>

### Citation

{% raw %}
```bibtex
@misc{deng2025distill,
  title   ={{Sequence Generative Adversarial Nets (SeqGAN)}},
  author  ={Wei Deng},
  journal ={waynedw.github.io},
  year    ={2025},
  howpublished = {\url{https://weideng.org/posts/seqGAN}}
}
```
{% endraw %}
