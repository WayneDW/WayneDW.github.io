---
title: 'Sequence GAN'
subtitle: Adversarial generations of discrete sequences 
date: 2025-11-29
permalink: /posts/seqGAN/
category: Sampling
---

Generative Adversarial Networks (GANs) {% cite goodfellow2014generative %} were once among the best generative models for image data. Their adversarial training enables the generator ($$\mathbf{G}$$) to produce high-fidelity samples that even a strong discriminator ($$\mathbf{D}$$) cannot distinguish. $$\mathbf{D}$$ and $$\mathbf{G}$$ play the minimax game:

$$
\begin{align}
& \mathrm{\min_{G} \max_{D} \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)]
+ \mathbb{E}_{z\sim G} [\log(1 - D(z))]}. \notag
\end{align}
$$

 Despite this success, however, its generation of **discrete sequences** remains poorly understood due to:

1. The loss of $$\mathbf{D}$$ from $$\mathbf{G}$$ samples is not **non-differentiable** on discrete data;
2. $$\mathbf{D}$$ cannot evaluate a **intermediate** state $$\mathrm{z_{1:t}}$$ unless $$\mathrm{t}$$ reaches the full sequence length $$\mathrm{T}$$. 


### Sequence Generative Adversarial Nets (SeqGAN)


SeqGAN resolves these issues by modeling the generator $$\mathbf{G_{\theta}}$$ as a policy model for sequence generation and using the discriminator $$\mathbf{D_{\phi}}$$ to provide the reward signal.

**Intermediate state $$\mathrm{z_{1:t}}$$**: SeqGAN performs Monte Carlo rollouts using the policy model $$\mathrm{G_{\theta}}$$ to sample the remaining steps such as 

$$
\begin{align}
& \mathrm{\{z_{1:T}^i\}_{i=1}^N \sim \text{Rollout}^{G_{\theta}}(z_{1:t}), \text{where}\ \  z_{1:t}^i=z_{1:t}}. \notag
\end{align}
$$

<figure style="text-align: center;">
    <img src="/images/seqGAN.png" width="550" height="200" />
    <figcaption> SeqGAN: $\mathbf{G}$ acts as a sequence-generation policy; $\mathbf{D}$ provides the rewards {% cite yu2017seqgan %}. </figcaption>
</figure>

**Non-differentiability**: Policy gradient methods {% cite williams1992simple %} are proposed to shift from the gradient of discrete actions to the probability space using the log-derivative trick:

$$
\begin{align}
& \nabla \mathrm{\mathbb{E}_{z_{t}\sim G_{\theta}(\cdot\mid z_{1:t-1})}[D_{\phi}^t(z_{1:t})]=\mathbb{E}_{z_{t}\sim G_{\theta}(\cdot\mid z_{1:t-1})}[D_{\phi}^t(z_{1:t}) \nabla_{\theta} \log G_{\theta}(z_t\mid z_{1:t-1})]}, \label{pg}
\end{align}
$$

where $$\mathrm{D_{\phi}^t}$$, where $$\mathrm{t\in \{1, 2, \cdots, T\}}$$, is the intermediate reward signal to be defined as:

$$
\begin{align}
& \mathrm{D^t_{\phi}(z_{1:t})} = \begin{cases}
\displaystyle 
\mathrm{\frac{1}{N} \sum_{i=1}^N D_{\phi}(z_{1:T}^n),
\quad z^{i}_{1:T} \sim \mathrm{Rollout}^{G_\theta}(z_{1:t})}, & \mathrm{t < T} \\[10pt]
\mathrm{D_\phi(z_{1:t})}, & \mathrm{t = T.}
\end{cases} \notag
\end{align}
$$

The optimization of $$\mathbf{G}$$ is equivalent to maximizing the reward using policy gradients, as implied by \eqref{pg}:

$$
\begin{align}
& \mathrm{R(\theta)=\sum_{t=1}^T \mathbb{E}_{z_{t}\sim G_{\theta}(\cdot\mid z_{1:t-1})}[D_{\phi}^t(z_{1:t})] \iff \nabla R(\theta)=\sum_{t=1}^T \mathbb{E}_{z_{t}\sim G_{\theta}(\cdot\mid z_{1:t-1})}[D_{\phi}^t(z_{1:t})\nabla_{\theta} \log G_{\theta}(z_t\mid z_{1:t-1})]}. \notag
\end{align}
$$






<!-- On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes

One-step Diffusion with Distribution Matching Distillation 
https://arxiv.org/pdf/2311.18828 -->

<!-- SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient -->

### Applications in On-policy Distillation

The above framework finds perfect applications in on-policy distillation {% cite ye2025blackbox %} {% cite zheng2025ultrafast %}. The generator (policy model) can be the student LLM (fine-tuned Qwen) or dLLM; the discriminator(reward model) is the tweaked generator model with an extra prediction head; GPT or LLaDA can be the teacher model.



<br><br><br>

### Citation

{% raw %}
```bibtex
@misc{deng2025seqGAN,
  title   ={{Sequence GAN}},
  author  ={Wei Deng},
  journal ={waynedw.github.io},
  year    ={2025},
  howpublished = {\url{https://weideng.org/posts/seqGAN}}
}
```
{% endraw %}
