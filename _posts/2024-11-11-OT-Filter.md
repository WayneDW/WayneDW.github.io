---
title: 'Optimal Transport of Nonlinear Filtering'
subtitle: Interpretating Bayes’ Law for Nonlinear Filtering
date: 2024-11-11
permalink: /posts/OT_filter/
category: State Space Model
---


Linear Filter . Sequential Monte Carlo (particle filter) effective in low dimensions. Inefficient and becomes degenerate in high dimensions. 

Bayes’ law gives the conditional distribution of the state X given observations Y according to xx


#### Preliminaries on Optimal Transportation Theory

Given two probability measures $\mu$ and $\nu$ on Rn, we say a map $T$ transports $\mu$ to $\nu$ if $T_{#} \mu=\nu$, where $#$ is the push-forward operator. 

The Monge optimal transportation problem with quadratic cost is to select the transport map from $\mu$ to $\nu$ with the least quadratic cost:

$$\min_{T\in \mathcal{T}(\mu, \nu)} \mathbb{E}_{Z\sim \mu} \bigg[\frac{1}{2} \| T(Z) - Z\|_2^2\bigg]$$

The Monge problem has a dual formulation (a.k.a. the Monge-Kantorovich (MK) dual problem)

$$\min_{f\in CVX(\mu)} \mathbb{E}_{U\sim \mu}[f(Z)] + \mathbb{E}_{V\sim \nu}[f^*(Z)] $$

where $f^*$ denotes the convex conjugate of $f$, i.e. $f^*(\nu)=\sup_{z\in \mathbb{R}^n} z^T \nu - f(z)$, and $CVX(\mu)$ denotes the set of all convex $\mu$ and $\mu$-integrable functions on $\mathbb{R}^n$. Then the MK dual problem above has a unique minimizer $\bar f$ and that $\bar T=\nabla \bar f$ is the solution to the Monge problem. The map $\bar T$ is often referred to as the Brenier transport map (solution).


#### A Variational Formulation of Bayes' Law

This section proposes a variational formulation of Bayes’ law (1), characterizing the posterior distribution as the pushforward of the prior via a parametric map. 


The key idea that resolves this issue is to instead consider a transport
problem on the product space $\mathbb{R}^n \times \mathbb{R}^m$ 

More precisely, find a map $(x,y)-> S(x, y)=(T(x,y), y)$ that transports the independent coupling $P_X \otimes P_Y$ to the joint distribution $P_{XY}$. The structure of the map $S(x,y)$ implies that its first component $T(x,y)$ serves as a transport map from $P_X$ to $P_{X|Y}$ yielding the new optimal transportation problem.

$$\min_{S\in \mathcal{T}(P_X \otimes P_Y, P_{XY})} \mathbb{E}\bigg[\| T(X,Y) - X \|_2^2 \bigg]$$,

The optimal transportation problem is numerically infeasible to solve.


$$\min_{f(\cdot, y)\in CVX(P_X)} J(f):=\mathbb{E}_{(X,Y)\sim P_X\otimes P_Y}[f(X, Y)] + \mathbb{E}_{(X,Y)\sim P_{XY}}[f^*(X, Y)],$$

where the constraint $f(\cdot, y)\in CVX(P_X)$ means that $x\rightarrow f(x;y)$ is convex and in $L^1(P_X)$ for any $y\in \mathbb{R}^m$. Similarly, $f^*(x, y)=\sup_z z^T x - f(z; y)$ is the convex conjugate of $f(\cdot; y)$ for fixed y.

### Computational Algorithms


The value of the objective function is easily approximated empirically. In particular,
given the ensemble of particles $\{X_0^i\}_{i=1}^N$ that form samples from the prior distribution $P_X$, one generates observations $Y_0^i\sim P_{Y|X}(\cdot | X_0^i)$ for $i=1,2,\cdots, N$ so that $\{(X_0^i, Y_0^i)\}_{i=1}^N$ form independent samples from the joint distribution $P_{XY}$. The samples are then used to define the empirical cost:

$$J^{(N)}(f):=\frac{1}{N^2} \sum_{i,j=1}^N f(X_0^i, Y_0^i) + \frac{1}{N} \sum_{i=1}^N f^*(X_0^i, Y_0^i)$$. It is straightforward to see that J(N)(f) is an unbiased estimator of $J(f)$. The empirical approximation is then utilized to formulate optimization problems of the form:

$$\min_{f\in \mathcal{F}} J^{(N)}(f)$$

where $\mathcal{F}$ is a subset of functions $f(x;y): \mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}$ such that $x \rightarrow f(x; y)$ is convex in $x$. 


#### Functional classes 


We discuss restriction of $\mathcal{F}$ to two class of functions, namely quadratic, and neural networks.

##### Optimal transport EnKF

Consider the class of quadratic functions in x,

$$\mathcal{F}_Q=\{(x,y)\rightarrow xx + xx | \},$$

where $S^n_+$ denotes the set of positive-definite matrices. Such functions give rise to linear transport maps of the form

$$\nabla_x f(x, y) = Ax + Ky + b,$$

Problem (4) is analogous to optimizing over the parameters of a quadratic function,
yields the optimization problem

$$\min_{\theta \in \Theta} \frac{1}{2} Tr(A\Sigma_x) + \frac{1}{2} Tr(A^{-1}\Sigma_x) + \frac{1}{2} Tr(A^{-1} K\Sigma_y K^\intercal) - Tr(A^{-1} \Sigma_{xy} K^\intercal) + \frac{1}{2}(\tilde b - m_x)^\intercal A^{-1} (\tilde b - m_x)$$