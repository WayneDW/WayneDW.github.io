<html>
<head>
    <title>The Conjoined Triangle of Flow, Diffusion, and PDE</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Wei Deng is a machine learning researcher and applied mathematician.'>
    <meta name='keywords' content=''>
    <meta name='author' content='Wei Deng'>

    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>

    
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [ ['$$', '$$'], ["\\[","\\]"]],
      tags: 'ams',
      displayAlign: "center"
    }
  };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/publications/'>Publications</a></li>
	<li><a href='/blog/'>Blog</a></li>
    </ul>
</div>

    <div class='front-matter'>
        <div class='wrap'>
            <h1>The Conjoined Triangle of Flow, Diffusion, and PDE</h1>
            <h4>Connections between Probability Flows, Diffusions, and PDEs.</h4>
            <div class='bylines'>
                <div class='byline'>
                    <!-- <h3>Published</h3> -->
                    <p>01 July 2023</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <!-- https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference -->

<h3 id="diffusion-process">Diffusion Process</h3>

<p>Consider a diffusion process that solves the Itô’s SDE <a class="citation" href="#oksendal2003stochastic">(Øksendal, 2003)</a>:</p>

\[\begin{align}
\mathrm{d} \mathrm{X}_t=\boldsymbol{\mathrm{v}}(\mathrm{X}_t, t) \mathrm{d} t + \sigma(X_t) \mathrm{d} \mathrm{W}_t.\notag
\end{align}\]

<p>Denote by $\mathrm{P}_t$ the transition function of Markov process</p>

\[\begin{align}
(\mathrm{P}_t f)(x)=\int f(y)\mathrm{P}(t, x, \mathrm{d} y)\notag.
\end{align}\]

<p>Define the generator $\mathscr{L}f=\lim \frac{\mathrm{P}_t f - f}{y}$, where $\mathrm{P}_t=e^{t \mathscr{L}}$. Analyzing the transition of the conditional expectation $\mathbb{E}(f(\mathrm{X}_t)|\mathrm{X}_s=x)$, we have the backward Kolmogorov equation</p>

\[\begin{align}
\mathscr{L}&amp;=\boldsymbol{\mathrm{v}}\cdot \nabla + \frac{1}{2} \Sigma:D^2\notag\\
           &amp;=\sum_{j=1}^d \mathrm{v}_j\frac{\partial}{\partial x_j} + \frac{1}{2}\sum_{i,j=1}^d \Sigma_{ij}\frac{\partial^2}{\partial x_i \partial x_j}\notag.
\end{align}\]

<p>where $\nabla$ and  $\nabla\cdot$ denote the gradient and the divergence in $\mathbb{R}^d$, $\Sigma(x)=\sigma(x) \sigma(x)^\intercal$ and $D^2$ denotes the Hessian matrix.</p>

<!-- # https://openreview.net/pdf?id=x9tAJ3_N0k -->

<h3 id="fokker-planck-equation">Fokker-Planck Equation</h3>

<p>Further define the semigroup $\mathrm{P}_t^{*}$ that acts on probability measures</p>

\[\begin{align}
\mathrm{P}^{*}_t \mu(\Gamma)=\int \mathrm{P}(t, x, \Gamma)\mathrm{d} \mu(x)\notag.
\end{align}\]

<p>The semigroup $\mathrm{P}_t$ and $\mathrm{P}_t^{*}$ are adjoint in $L^2$ such that</p>

\[\begin{align}
\int (\mathrm{P}_t f)\mathrm{d}\mu=\int f\mathrm{d}(\mathrm{P}_t^{*}\mu)\notag,
\end{align}\]

<p>which yields</p>

\[\begin{align}
\int \mathscr{L} f h \mathrm{d} x = \int f \mathscr{L}^* h \mathrm{d}x, \text{ where } \mathrm{P}_t^*=e^{t {\mathscr{L}}^*}.\notag
\end{align}\]

<p>Let $p_t$ denote the law of the Markov process at time $t$. The law of $p_t$ follows that</p>

\[\begin{align}
\frac{\partial p_t}{\partial t} =\mathscr{L}^* p_t,\notag
\end{align}\]

<p>which is the Fokker-Planck equation. Analyzing the evolution of the probability densities, we have</p>

\[\begin{align}
\mathscr{L}^{*} p_t&amp;=\nabla \cdot \bigg(-\boldsymbol{\mathrm{v}} p_t + \frac{1}{2} \nabla\cdot\big(\Sigma p_t\big)\bigg). \notag \\
                &amp;=\nabla \cdot \bigg(-\boldsymbol{\mathrm{v}} p_t + \frac{1}{2} \big(\nabla\cdot \Sigma\big) p_t + \frac{1}{2} \Sigma \nabla p_t \bigg). \notag \\
                &amp;=\nabla \cdot \bigg(-\underbrace{\bigg(\boldsymbol{\mathrm{v}} - \frac{1}{2} \big(\nabla\cdot \Sigma\big) - \frac{1}{2} \Sigma \nabla \log p_t\bigg)}_{\boldsymbol{\nu}_t} p_t \bigg), \notag \\
\end{align}\]

<p>where the last equality follows by $\nabla \log p_t = \frac{\nabla p_t}{p_t}$.</p>

<h3 id="probability-flow">Probability Flow</h3>

<p>Denote by $\boldsymbol{\nu}=\boldsymbol{\mathrm{v}} - \frac{1}{2} \big(\nabla\cdot \Sigma\big) - \frac{1}{2} \Sigma \nabla \log p$, the FPE is recased as the transport equation <a class="citation" href="#OT_applied_math">(Santambrogio, 2015)</a> or continuity equation in fluid dynamics <a class="citation" href="#log_concave_sampling">(Chewi, 2023)</a>.</p>

\[\begin{align}
\partial_t p_t =- \nabla \cdot ({\boldsymbol{\nu_t}} p_t).\notag
\end{align}\]

<p>Interestingly, it corresponds to the probability flow ODE <a class="citation" href="#score_sde">(Song et al., 2021)</a></p>

\[\begin{align}
\mathrm{d} X_t={\boldsymbol{\nu_t}}(X_t) \mathrm{d} t.\notag
\end{align}\]

<p>Apply the instantaneous change of variables <a class="citation" href="#neural_ode">(Chen et al., 2018)</a>, the <strong>log-likelihood</strong> of $p_0(x)$ follows that</p>

\[\begin{align}
\log p_0(x)=\log p_T(x) + \int_0^T \nabla \cdot {\boldsymbol{\nu_t}} \mathrm{d} t,\notag
\end{align}\]

<p>which provides an elegant way to compute the likelihood for diffusion models.</p>

<p>In practice, it is expensive to evaluate the divergence $\nabla \cdot {\boldsymbol{\nu_t}}$. We can adopt the Hutchinson trace estimator <a class="citation" href="#Hutchinson89">(Hutchinson, 1989)</a>.</p>

\[\begin{align}
\nabla \cdot {\boldsymbol{\nu_t}} = \mathbb{E} \big[\epsilon^\intercal \nabla {\boldsymbol{\nu_t}} \epsilon \big],
\end{align}\]

<p>where $\nabla {\boldsymbol{\nu_t}}$ is the Jaconbian of ${\boldsymbol{\nu_t}}$; the random variable $\epsilon$ is a standard Gaussian vector and $\epsilon^\intercal \nabla {\boldsymbol{\nu_t}}$ can be efficiently computed using reverse-mode automatic differentiation.</p>


    </div>
    <div id='bibliography'>
        <div class='wrap'>
            <ol class="bibliography"><li><span id="oksendal2003stochastic">Øksendal, B. (2003). <i>Stochastic Differential Equations: An Introduction with Applications</i>. Springer.</span></li>
<li><span id="OT_applied_math">Santambrogio, F. (2015). <i>Optimal Transport for Applied Mathematicians: 
 Calculus of Variations, PDEs, and Modeling</i>. Springer.</span></li>
<li><span id="log_concave_sampling">Chewi, S. (2023). <i>Log-Concave Sampling</i>. Online Draft.</span></li>
<li><span id="score_sde">Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations
. <i>ICLR</i>.</span></li>
<li><span id="neural_ode">Chen, R. T. Q., Rubanova, Y., Bettencourt, J., &amp; Duvenaud, D. (2018). Neural Ordinary Differential Equations. <i>NeurIPS</i>.</span></li>
<li><span id="Hutchinson89">Hutchinson, M. F. (1989). A Stochastic Estimator of the Trace of the Influence Matrix for Laplacian
Smoothing Splines. <i>Communications in Statistics-Simulation and Computation</i>, <i>18</i>(3), 1059–1076.</span></li></ol>
        </div>
    </div>
</div>
</body>
</html>
