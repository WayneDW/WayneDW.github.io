<html>
<head>
    <title>Dynamic Importance Sampling and Beyond</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Wei Deng is a machine learning researcher and applied mathematician.'>
    <meta name='keywords' content=''>
    <meta name='author' content='Wei Deng'>

    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>

    
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [ ['$$', '$$'], ["\\[","\\]"]],
      tags: 'ams',
      displayAlign: "center"
    }
  };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/publications/'>Publications</a></li>
	<li><a href='/blog/'>Blog</a></li>
    </ul>
</div>

    <div class='front-matter'>
        <div class='wrap'>
            <h1>Dynamic Importance Sampling and Beyond</h1>
            <h4>The most promising sampling framework for multi-modal simulations.</h4>
            <div class='bylines'>
                <div class='byline'>
                    <!-- <h3>Published</h3> -->
                    <p>05 November 2020</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>Point estimation tends to over-predict out-of-distribution samples <a class="citation" href="#Balaji17">(Lakshminarayanan et al., 2017)</a> and leads to unreliable predictions. Given a cat-dog classifier, can we predict flamingo as the <strong>unknown</strong> class?</p>

<p align="center">
    <img src="/images/cat_dog.png" width="400" />
</p>

<p>The key to answering this question is <strong>uncertainty</strong>, which is still an open question. Yarin gave a good tutorial on uncertainty predictions using Dropout <a class="citation" href="#YARIN_GAL_blog">(Gal, n.d.)</a>. However, that method tends to underestimate uncertainty due to the nature of variational inference.</p>

<h2 id="importance-sampling">Importance sampling</h2>
<p>How can we give efficient uncertainty quantification for deep neural networks? To answer this question, we first show a baby example. Suppose we are interested in a Gaussian mixture distribution, the standard stochastic gradient Langevin dynamics <a class="citation" href="#Welling11">(Welling &amp; Teh, 2011)</a> suffers from the local trap issue.</p>

<p align="center">
    <img src="/images/original_density.png" width="250" height="250" />
</p>

<p>To tackle that issue and accelerate computations, we consider importance sampling</p>

<p align="center">
    <img src="/images/importance_sampling.png" width="600" height="90" />
</p>

<p>That is when the original density is hard to simulate, but the new density is easier. Together with the importance weight, we can obtain an estimate indirectly by sampling from a new density.</p>

<h2 id="build-a-flattened-density">Build a flattened density</h2>

<p>What kind of distribution is easier than the original? A <strong>flattened</strong> distribution!</p>

<p align="center">
    <img src="/images/flat_density.png" width="300" height="300" />
</p>

<p>How to build such a flat density? One famous example is <a href="https://arxiv.org/pdf/physics/9803008.pdf">annealed importance sampling</a> via high temperatures; another (ours) is to exploit ideas from <a href="https://en.wikipedia.org/wiki/Wang_and_Landau_algorithm">Wang-Landau algorithm</a> and divide the original density by the <strong>energy PDF</strong>.</p>
<p align="center">
    <img src="/images/energyPDF.png" width="600" height="60" />
</p>

<p>Given the energy PDF, we can enjoy a <strong>random walk</strong> in the <strong>energy space</strong>. Moreover, the bias caused by simulating from a different density can be adjusted by the importance weight.</p>

<h2 id="sample-trajectory-in-terms-of-learning-rates">Sample trajectory in terms of learning rates</h2>
<p>CSGLD possesses a self-adjusting mechanism to escape local traps. Most notably, it leads to <strong>smaller or even negative learning rates in low energy regions to bounce particles out</strong>.</p>

<p align="center">
    <img src="/images/moves.png" width="700" height="200" />
</p>

<h2 id="estimate-the-energy-pdf-via-stochastic-approximation">Estimate the energy PDF via stochastic approximation</h2>
<p>Since we don’t know the energy PDF in the beginning, we can adaptively estimate it on the fly via <strong>stochastic approximation</strong>. In the long run, we expect that the energy PDF is gradually estimated and we can eventually simulate from the target flat density. Theoretically, this algorithm has a stability property such that the <strong>estimate of energy PDF converges to a unique fixed point regardless of the non-convexity</strong> of the energy function.</p>

<p>The following is a demo to show how the energy PDF is estimated. In the beginning, CSGLD behaves similarly to SGLD. But soon enough, it moves quite <strong>freely</strong> in the energy space.</p>

<p float="left" align="center">
  <img src="/images/CSGLD/CSGLD_with_PDF.gif" width="200" title="SGLD" />
  <img src="/images/CSGLD/CSGLD_PDF.gif" width="200" alt="Made with Angular" title="Angular" /> 
</p>

<p>The following result shows <a href="https://github.com/WayneDW/Contour-Stochastic-Gradient-Langevin-Dynamics/blob/master/CSGLD_demo.ipynb">[code]</a> what the flattened and reweighted densities look like.</p>

<p align="center">
    <img src="/images/resample.png" width="600" height="210" title="A mixture example with 9 modes" />
</p>

<h2 id="comparison-with-other-methods">Comparison with other methods</h2>
<p>We compare CSGLD <a class="citation" href="#CSGLD">(Deng et al., 2020)</a> with SGLD <a class="citation" href="#Welling11">(Welling &amp; Teh, 2011)</a>, <a class="citation" href="#ruqi2020">(Zhang et al., 2020)</a>, and <a class="citation" href="#deng2020">(Deng et al., 2020)</a>, and observe that CSGLD is comparable to reSGLD and faster than SGLD and cycSGLD.</p>
<p float="left">
  <img src="/images/CSGLD/SGLD.gif" width="170" title="SGLD" />
  <img src="/images/CSGLD/cycSGLD.gif" width="170" alt="Made with Angular" title="Angular" />
  <img src="/images/CSGLD/reSGLD.gif" width="170" alt="hello!" title="adam solomon's hello" />
  <img src="/images/CSGLD/CSGLD.gif" width="170" />
</p>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th style="text-align: center">Special features</th>
      <th style="text-align: center">Cost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SGLD (ICML’11)</td>
      <td style="text-align: center">None</td>
      <td style="text-align: center">None</td>
    </tr>
    <tr>
      <td>Cycic SGLD (ICLR’20)</td>
      <td style="text-align: center">Cyclic learning rates</td>
      <td style="text-align: center">More cycles</td>
    </tr>
    <tr>
      <td>Replica exchange SGLD (ICML’20)</td>
      <td style="text-align: center">Swaps/Jumps</td>
      <td style="text-align: center">Parallel chains</td>
    </tr>
    <tr>
      <td>Contour SGLD (NeurIPS’20)</td>
      <td style="text-align: center">Bouncy moves</td>
      <td style="text-align: center">Latent vector</td>
    </tr>
  </tbody>
</table>

<h2 id="summary">Summary</h2>
<p>Contour SGLD can be viewed as a scalable Wang-Landau algorithm in deep learning. It paves the way for future research in various adaptive biasing force techniques for big data problems. We are working on extensions of this algorithm in both theory and large-scale AI applications. If you like this paper, you can cite</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{CSGLD,
  title={A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions},
  author={Wei Deng and Guang Lin and Faming Liang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
</code></pre></div></div>

<p>For Chinese readers, you may also find this blog interesting <a href="https://zhuanlan.zhihu.com/p/267633636">知乎</a>.</p>


    </div>
    <div id='bibliography'>
        <div class='wrap'>
            <ol class="bibliography"><li><span id="Balaji17">Lakshminarayanan, B., Pritzel, A., &amp; Blundell, C. (2017). Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble. <i>NeurIPS</i>.</span></li>
<li><span id="YARIN_GAL_blog">Gal, Y. What My Deep Model Doesn’t Know... <i>Blog</i>.</span></li>
<li><span id="Welling11">Welling, M., &amp; Teh, Y. W. (2011). Bayesian Learning via Stochastic Gradient Langevin Dynamics. <i>ICML</i>.</span></li>
<li><span id="CSGLD">Deng, W., Lin, G., &amp; Liang, F. (2020). A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions. <i>Neurips</i>.</span></li>
<li><span id="ruqi2020">Zhang, R., Li, C., Zhang, J., Chen, C., &amp; Wilson, A. G. (2020). Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning. <i>ICLR</i>.</span></li>
<li><span id="deng2020">Deng, W., Feng, Q., Gao, L., Liang, F., &amp; Lin, G. (2020). Non-Convex Learning via Replica Exchange Stochastic Gradient MCMC. <i>ICML</i>.</span></li></ol>
        </div>
    </div>
</div>
</body>
</html>
