<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-16T20:12:42-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Wei Deng / 邓伟</title><subtitle>Ph.D. candidate in Applied Math at Purdue University</subtitle><author><name>Wei Deng</name></author><entry><title type="html">Autoregressive Flow</title><link href="http://localhost:4000/posts/autoregressive_flow/" rel="alternate" type="text/html" title="Autoregressive Flow" /><published>2024-03-16T00:00:00-07:00</published><updated>2024-03-16T00:00:00-07:00</updated><id>http://localhost:4000/posts/ar-flow</id><content type="html" xml:base="http://localhost:4000/posts/autoregressive_flow/"><![CDATA[### Normalizing Flows

Normalizing flows {% cite normalzing_flow_jmlr %} are the pioneers in generative models. The main idea is to transform a random vector $\mathrm{u}$ through an invertible transport map $\mathrm{T}$:

$$\begin{align}
  \mathrm{x}=\mathrm{T}(\mathrm{u}), \ \ \text{where}\  \mathrm{u}\sim \mathrm{p}_{\mathrm{u}}(\mathrm{u}).\notag
\end{align}$$

If $\mathrm{T}$ is bijective and differentiable, the density of $\mathrm{x}$ can be obtained by a change of variables

$$\begin{align}
  \mathrm{p}_{\mathrm{X}}(\mathrm{x})=\mathrm{p}_{\mathrm{u}}(\mathrm{u})\bigg|\text{det} \mathrm{J}_{\mathrm{T}}(\mathrm{u}) \bigg|^{-1}=\mathrm{p}_{\mathrm{u}}(\mathrm{T}^{-1}(\mathrm{X}))\bigg|\text{det} \mathrm{J}_{\mathrm{T}^{-1}}(\mathrm{u}) \bigg|.\notag
\end{align}$$

where $$\text{det}\ \mathrm{J}_{\mathrm{T}}(\mathrm{u})$$ quantifies the change of volume w.r.t. u, such that $$\text{det}\ \mathrm{J}_{\mathrm{T}}(\mathrm{u}) \approx \frac{\text{Vol}(\mathrm{d} \mathrm{x})}{\text{Vol}(\mathrm{d} \mathrm{u})}$$; the second equality holds since $\text{det} (\mathrm{A} \mathrm{A}^{-1})=1=\text{det} (\mathrm{A}) \text{det}(\mathrm{A}^{-1})$.


#### Composition Property

To model complex distributions, it is standard to conduct multiple transformations $\mathrm{T}_1$, $\mathrm{T}_2, \cdots$, $\mathrm{T}_K$ such that $\mathrm{T}=\mathrm{T}_K \circ \cdots \circ \mathrm{T}_2 \circ \mathrm{T}_1$. If $$\{\mathrm{T}_k\}_{k=1}^K$$ are also bijective and invertible, we have

$$\begin{align}
  \mathrm{T}^{-1}&=(\mathrm{T}_K \circ \cdots \circ \mathrm{T}_2 \circ \mathrm{T}_1)^{-1}=\mathrm{T}_1^{-1} \circ \mathrm{T}_2^{-1} \circ \dots \mathrm{T}_K^{-1}.\label{iterative_maps} \\
  \text{det} \ \mathrm{J}_{\mathrm{T}}(\mathrm{u})& = \text{det} \mathrm{J}_{\mathrm{T}_K \circ \dots \mathrm{T}_1}(\mathrm{u}) = \text{det} \mathrm{J}_{\mathrm{T}_1}(\mathrm{u})  \cdot \text{det} \mathrm{J}_{\mathrm{T}_2}(\mathrm{T}_1(\mathrm{u}))  \cdot \text{det} \mathrm{J}_{\mathrm{T}_K}(\mathrm{T}_{K-1}(\cdots(\mathrm{T}_1(\mathrm{u})))).\notag
\end{align}$$


#### KL Divergence


Denote by  $\phi$ and $\psi$ the parameters of $\mathrm{T}$ and $\mathrm{p}_{\mathrm{u}}(\mathrm{u})$, respectively. The forward KL divergence follows that

$$\begin{align}
  \mathrm{L}(\phi, \psi)&=\text{KL}(\mathrm{p}^{\star}_{\mathrm{X}}(\mathrm{x})\| \mathrm{p}_{\mathrm{X}}(\mathrm{x}; \phi, \psi)) \label{forward_KL} \\
  &=-\mathbb{E}_{\mathrm{p}^{\star}_{\mathrm{X}}(\mathrm{x})} [\log \mathrm{p}_{\mathrm{X}}(\mathrm{x}; \phi, \psi)] + \text{const} \notag \\
  &=-\mathbb{E}_{\mathrm{p}^{\star}_{\mathrm{X}}(\mathrm{x})} [\log \mathrm{p}_{\mathrm{u}}(\mathrm{T}^{-1}(\mathrm{x}; \phi); \psi) + \log |\text{det} \mathrm{J}_{\mathrm{T}^{-1}}(\mathrm{x}; \phi)|]+\text{const}. \notag
\end{align}$$

The forward KL divergence is well-suited to generate the data distribution if we can model the transport maps $\mathrm{T}$ efficiently as shown in the masked autoencoder below, but is less efficient in computing the model density $\mathrm{p}^{\star}_{\mathrm{X}}(\mathrm{x})$ due to the iterative maps in Eq.\eqref{iterative_maps}.


Alternatively, when we are interested to evaluate model density $$\mathrm{p}_{\mathrm{X}^{\star}}(\mathrm{x})$$, we can try to optimize the reverse KL divergence 

$$\begin{align}
  \mathrm{L}(\phi, \psi)&=\text{KL}(\mathrm{p}_\mathrm{X}(\mathrm{x}; \phi, \psi) \| \mathrm{p}^{\star}_{\mathrm{X}}(\mathrm{x})) \label{reverse_KL} \\
  &=\mathbb{E}_{\mathrm{p}_{\mathrm{X}}(\mathrm{x}; \phi, \psi)} [\log \mathrm{p}_{\mathrm{X}}(\mathrm{x}; \phi, \psi) - \log \mathrm{p}_{\mathrm{X}}^{\star}(\mathrm{x})]  \notag \\
  &=-\mathbb{E}_{\mathrm{p}_{\mathrm{u}}(\mathrm{u}; \psi)} [\log \mathrm{p}_{\mathrm{u}}(\mathrm{u}; \psi) - \log |\text{det} \mathrm{J}_{\mathrm{T}}(\mathrm{u}; \phi)| - \log \mathrm{p}_{\mathrm{X}}^{\star}(\mathrm{T}(\mathrm{u}; \phi))]. \notag
\end{align}$$

In empirical training, we resrot to Monte Carlo samples to approximate the expectation.


### Autogressive Flows

Autoregressive flows represent one of the earliest developments in flow-based models. We can map the data distribution $\mathrm{p}_\mathrm{X}(\mathrm{x})$ into a uniform distribution in $(0, 1)^\mathrm{D}$ using a transport map with a triangular Jacobian. 

To achieve the goal of universal representation, we leverage conditional probability and decompose $\mathrm{p}_\mathrm{X}(\mathrm{x})$ into a product of conditional probabilities as follows

$$\begin{align}
  \mathrm{p}_{\mathrm{X}}(\mathrm{x}) = \Pi_{i=1}^\mathrm{D} \mathrm{p}_{\mathrm{X}}(\mathrm{x}_i | \mathrm{x}_{<i}).\label{seq_structure}
\end{align}$$

Define the transformation $\mathrm{F}$ to be the CDF function of the conditional density:

$$\begin{align}
  \mathrm{z}_i = \mathrm{F}_i(\mathrm{x}_i| \mathrm{x}_{<i})=\int_{-\infty}^{\mathrm{x}_i} \mathrm{p}_{\mathrm{X}}(\mathrm{x}_i'|\mathrm{x}_{<i}) \mathrm{d} \mathrm{x}_i'=\text{P}(\mathrm{x}_i'\leq \mathrm{x}_i | \mathrm{x}_{<i}).\notag
\end{align}$$

The invertibility of $\mathrm{F}$ leads to 

$$\begin{align}
  \mathrm{x}_i = (\mathrm{F}_i(\cdot|\mathrm{x}_{<i}))^{-1} (\mathrm{z}_i) \text{ for } i=1,2,..., \mathrm{D}.\notag
\end{align}$$

Notably, the Jacobian of $\mathrm{F}$ is a lower trianagular matrix and the determinant of $\mathrm{F}$ is equal to the product of diagonal elemants.

$$\begin{align}
  \text{det} \mathrm{J}_\mathrm{F}(\mathrm{x}) = \Pi_{i=1}^\mathrm{D} \frac{\partial \mathrm{F}_i}{\partial \mathrm{x}_i} = \Pi_{i=1}^\mathrm{D} \mathrm{p}_\mathrm{X}(\mathrm{x}_i | \mathrm{x}_{<i})=\mathrm{p}_\mathrm{X}(\mathrm{x}).\notag
\end{align}$$

If there is another map $\mathrm{G}$ that transforms a general prior distribution to the uniform distribution $(0, 1)^\mathrm{D}$, the flow with transformation $\mathrm{T}=\mathrm{F}^{-1}\circ \mathrm{G}$ can map a general prior to the data distribution.


### Modeling

#### Masked Autoencoder

To model the sequential structure \eqref{seq_structure} via an efficient parallelism, {% cite mask_ae %} abandoned the RNN encoder and proposed a mind-blowing idea by using an masked autoencoder. The code snippet is from {% cite torch_nf %}.

```python
class MaskedLinear(nn.Linear):
    """ apply masks to a standard MLP """
    def __init__(self, in_features, out_features, bias=True):
        super().__init__(in_features, out_features, bias)        
        self.register_buffer('mask', torch.ones(out_features, in_features))
        
    def set_mask(self, mask):
        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))
        
    def forward(self, input):
        return F.linear(input, self.mask * self.weight, self.bias)
```

The challenge is to design proper masks to satisfy the autoregressive (triangular) property. A good tutorial is provided below {% cite masked_ae_youtube_tutorial %}. 

<p align="center">
    <img src="/images/masked_ae.png" width="500" />
</p>

To ensure the output $\tilde x_d$ only depend on the preceding inputs $x_{<d}$.

1. Assign the $k$-th hidden unit some random number $m(k)$ in $\\{1, 2, \cdots, D-1\\}$, which is the maximum number of input units that can be connected.

```python
    # sample the order of the inputs and the connectivity of all neurons
    self.m[-1] = np.arange(self.nin) if self.natural_ordering else rng.permutation(self.nin)
    for l in range(L):
        self.m[l] = rng.randint(self.m[l-1].min(), self.nin-1, size=self.hidden_sizes[l])
```
2. For any layers except the last one, the mask value is $$1_{m(k)\geq d}$$ for the $d$-th output; The sign is changed to $>$ for the last layer. 

```python
    # construct the mask matrices
    masks = [self.m[l-1][:,None] <= self.m[l][None,:] for l in range(L)]
    masks.append(self.m[L-1][:,None] < self.m[-1][None,:])
```


#### Masked Autogressive Flow (MAF)

MAF {% cite MAF %} propose to optimize the reverse KL objective {% cite reverse_KL %}, which is efficient in the conditional density estimation while the sampling stage is slow in high dimensions because we need to iterate the dimension sequentially. To help understand the masked autoregressive flow, I simplified {% cite torch_nf %}'s code template and added some edits and comments.

```python
from nflib.made import ARMLP

class MAF(nn.Module):
    """ Masked Autoregressive Flow that uses a MADE-style network for fast forward """
    def __init__(self, dim, parity, net_class=ARMLP, nh=24):
        super().__init__()
        self.dim = dim
        self.net = net_class(dim, dim*2, nh)
        self.parity = parity

    def forward(self, x):
        s, t = self.net(x)
        z = x * torch.exp(s) + t
        # permute elements for efficiency - pg5 https://arxiv.org/pdf/1705.07057.pdf
        z = z.flip(dims=(1,)) if self.parity else z
        log_det = torch.sum(s, dim=1)
        return z, log_det
    
    def backward(self, z):
        x = torch.zeros_like(z)
        log_det = 0 
        z = z.flip(dims=(1,)) if self.parity else z
        for i in range(self.dim): # iterative sampling from d=1 to D
            s, t = self.net(x.clone()) # clone to avoid in-place op errors if using IAF
            x[:, i] = (z[:, i] - t[:, i]) * torch.exp(-s[:, i])
            log_det += -s[:, i]
        return x, log_det

class NormalizingFlowModel(nn.Module):
    """ A Normalizing Flow Model is a (prior, flow) pair """
    def __init__(self, prior, af_flows):
        super().__init__()
        self.prior = prior
        self.af_flow = nn.ModuleList(af_flows)
        
    def iterate(self, data, direction=1):
        log_det = torch.zeros(data.shape[0])
        for af_flow in self.af_flow[::direction]:
            cur_flow = af_flow.forward if direction == 1 else af_flow.backward
            data, ld = cur_flow(data)
            log_det += ld
        return data, log_det
    
    def forward(self, x):
        z, log_det = self.iterate(x, direction=+1)
        prior_logprob = self.prior.log_prob(z).view(x.size(0), -1).sum(1)
        return z, prior_logprob, log_det

    def backward(self, z):
        return self.iterate(z, direction=-1)
    
    def sample(self, num_samples):
        z = self.prior.sample((num_samples,))
        x, _ = self.iterate(z, direction=-1)
        return x
```




#### Inverse Autogressive Flow (IAF)

Similar to MAF, IAF {% cite IAF %} proposed to flip the backward and forward sampling to speed up the sampling efficiency. The density estimation, however, becomes slower. The optimization corresponds to the forward KL divergence.

```python
class IAF(MAF):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.forward, self.backward = self.backward, self.forward
```]]></content><author><name>Wei Deng</name></author><category term="Flow" /><summary type="html"><![CDATA[Normalizing Flows]]></summary></entry><entry><title type="html">Ensemble Kalman Filter</title><link href="http://localhost:4000/posts/ensemble_kalman_filter/" rel="alternate" type="text/html" title="Ensemble Kalman Filter" /><published>2024-03-16T00:00:00-07:00</published><updated>2024-03-16T00:00:00-07:00</updated><id>http://localhost:4000/posts/ensemble-kalman-filter</id><content type="html" xml:base="http://localhost:4000/posts/ensemble_kalman_filter/"><![CDATA[]]></content><author><name>Wei Deng</name></author><category term="Filter" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Rectified Flow and Beyond</title><link href="http://localhost:4000/posts/rec_flow/" rel="alternate" type="text/html" title="Rectified Flow and Beyond" /><published>2024-03-09T00:00:00-08:00</published><updated>2024-03-09T00:00:00-08:00</updated><id>http://localhost:4000/posts/rec-flow</id><content type="html" xml:base="http://localhost:4000/posts/rec_flow/"><![CDATA[Diffusion models have demonstrated interesting applications, such as text-to-image generation (stable diffusion, DALLE), text-to-video generation (Sora), and text-to-audio generation. However, diffusion models often require tens or hundreds of steps in generation and is quite slow.

### Rectified Flow

To propose more efficient transport, rectified flow (RecFlow)  {% cite rec_flow %} proposes to straighten the flow to enable fewer discretization steps and has been adopted by stability AI {% cite stability_ai_transformer %}. For a given coupling $(\mathrm{X}_0, \mathrm{X}_1)\sim \pi_0\otimes \pi_1$, RecFlow proposes to solve the following probability-flow ODE 

$$\begin{align}
    \mathrm{d} \mathrm{Z}_t = \nu_t^{\star} (\mathrm{Z}_t) \mathrm{d} t, \label{ODE}
\end{align}$$
where $t\in[0, 1]$, and $\mathrm{Z}_0\sim \mathrm{X}_0$. $\nu_t^{\star}$ is the velocity field and is the solution of the objective

$$\begin{align}
    \inf_{\nu} \int_0^1 \mathbb{E}\bigg[ \big\|\mathrm{X}_1 - \mathrm{X}_0 - \nu_t(\mathrm{X}_t, t) \big\|_2^2 \bigg] \mathrm{d} t.\label{obj}
\end{align}$$

where $\mathrm{X}_t = t \mathrm{X}_0 + (1-t) \mathrm{X}_1$. The solution of Eq.\eqref{obj} is a straight line interpolation between $\mathrm{X}_0$ and $\mathrm{X}_1$ such that


$$\begin{align}
    \nu_t^{\star}(z)=\mathbb{E}\bigg[ \mathrm{X}_1 - \mathrm{X}_0 | \mathrm{X}_t = z\bigg].\label{velocity_field}
\end{align}$$

To obtain a valid coupling $(\mathrm{Z}_0, \mathrm{Z}_1)$, we can draw $\mathrm{Z}_1$ by simulating RecFlow \eqref{ODE} with $\mathrm{Z}_0\sim \pi_0$. 

#### Lower Transport Costs

For any convex transport cost function $c(\cdot)$, such as $\\|\cdot\\|_2^2$. We show the rectified coupling $(\mathrm{Z}_0, \mathrm{Z}_1)$ yields a lower transport cost in that $\mathbb{E}\big[c(\mathrm{Z}_1 - \mathrm{Z}_0) \big]\leq \mathbb{E}\big[c(\mathrm{X}_1 - \mathrm{X}_0) \big]$.

$$\begin{align}\notag
    \mathbb{E}\big[c(\mathrm{Z}_1 - \mathrm{Z}_0) \big] &= \mathbb{E}\bigg[c\bigg(\int_0^1 \nu_t^{\star}(\mathrm{Z}_t)\mathrm{d} t \bigg) \bigg] \\ \notag
    & \leq \mathbb{E}\bigg[\int_0^1 c(\nu_t^{\star}(\mathrm{Z}_t)) \mathrm{d} t \bigg] \\ \notag
    & = \mathbb{E}\bigg[\int_0^1 c(\nu_t^{\star}(\mathrm{X}_t)) \mathrm{d} t \bigg] \\ \notag
    & = \mathbb{E}\bigg[\int_0^1 c(\mathbb{E}\big[(\mathrm{X}_1 - \mathrm{X}_0 \big| \mathrm{X}_t)\big]) \mathrm{d} t \bigg] \\ \notag
    & \leq \mathbb{E}\bigg[\int_0^1 \mathbb{E}\big[c(\mathrm{X}_1 - \mathrm{X}_0) \big| \mathrm{X}_t\big] \mathrm{d} t \bigg] \\ \notag
    & = \int_0^1 \mathbb{E}\big[c(\mathrm{X}_1 - \mathrm{X}_0)\big]\mathrm{d} t \\ \notag
    & = \mathbb{E}\big[c(\mathrm{X}_1 - \mathrm{X}_0) \big].
\end{align}$$

where the first equality follows by Eq.\eqref{ODE} and the two inequalities follow by Jensen’s inequality. The second equality holds since Law($\mathrm{X}_t$)=Law($\mathrm{Z}_t$). The nice property also motivates to iterate this procedure to keep reducing the transport cost.


Nevertheless, straight interpolation is a necessary condition for optimal transport when the transport cost is the Euclidean distance {% cite Optimal_transport %}, but it is not sufficient because the couplings $(\mathrm{Z}_0, \mathrm{Z}_1)$ are also subject to optimize. 


#### More general cost functions

The quadratic transport cost function corresponds to the Brownian motion prior. For more general convex cost function $c$, the vector field {% cite rec_flow_general %} follows that

$$\begin{align}
   \nu_t(\mathrm{Z})=\nabla c^{\star} (\nabla f_t (\mathrm{Z})),\notag
\end{align}$$

where $c^{\star}$ is the conjugate function of $c$, $f$ is the solution of the Bregman divergence

$$\begin{align}
    \inf_f \int_0^1 \mathbb{E}\bigg[c^{\star}(\nabla f(\mathrm{X}_t)) - (\mathrm{X}_1 - \mathrm{X}_0)^\intercal \nabla f(\mathrm{X}_t) + c(\mathrm{X}_1 - \mathrm{X}_0) \bigg] \mathrm{d} t.\notag
\end{align}$$

The new path $\{\mathrm{Z}_t: t\in[0, 1]\}$ can be simulated via $\mathrm{d} \mathrm{Z}_t=\nabla c^{\star}(\nabla f_t(\mathrm{Z}_t))\mathrm{d}t$ with $\mathrm{Z}_0=\mathrm{X}_0$.



#### Comments:

**Straight flow with optimized coupling**: Similar to flow matching {% cite flow_match %}, RecFlow has a fairly user-friendly training loss via a quadratic cost and is appealing for straight-path simulation. The iterative rectify procedure further optimizes the coupling and makes the transport more efficient.

**Formulation**: The extension to general transport cost function looks a bit more complex than the Schrödinger bridge formulation, where the latter {% cite forward_backward_SDE %} is really elegant for optimizing general (convex/ non-convex) transport cost functions.

**Flow or diffusion**: Similar to SGD, RecFlow utilizes the randomness in mini-batch simulation and appears to be scalable for real-world datasets. This brings us a question if the manually injected noise via diffusion models are really needed. For example, stochastic interpolant {% cite stochastic_interpolant %} does unify flow and diffusion, but the training loss becomes implicit again, which may affect the scalability. In my opinion, diffusion model should still outperform flow models in training due to the ease of annealing. 

**Straight v.s. non-straight flows**: I am not fully convinced if displacement interpolantion is always the goal for general non-linear transport due to energy barriers. Empirically, we often observe a decent model with 12-16 NFEs, but a much worse model when we further decrease NFE and we have to resort to hacky tricks and distillation. Such empirical findings also motivate the study of {% cite multi_consistency_model %}.]]></content><author><name>Wei Deng</name></author><category term="Flow" /><summary type="html"><![CDATA[Diffusion models have demonstrated interesting applications, such as text-to-image generation (stable diffusion, DALLE), text-to-video generation (Sora), and text-to-audio generation. However, diffusion models often require tens or hundreds of steps in generation and is quite slow.]]></summary></entry><entry><title type="html">Implicit Ridge Regularization</title><link href="http://localhost:4000/posts/implicit_ridge_regularization/" rel="alternate" type="text/html" title="Implicit Ridge Regularization" /><published>2024-03-07T00:00:00-08:00</published><updated>2024-03-07T00:00:00-08:00</updated><id>http://localhost:4000/posts/implicit-ridge</id><content type="html" xml:base="http://localhost:4000/posts/implicit_ridge_regularization/"><![CDATA[### Ridge Regression with $n\gg d$: Bias-variance trade-off

We study the ordinary linear regression

$$\begin{align}
  \mathcal{L}=\|\mathrm{y}-\mathrm{X} {\beta}\|_2^2.\notag\\
\end{align}$$

where $\mathrm{X}\in \mathbb{R}^{n\times d}$ and is full rank. The solution follows that $\widehat \beta=(\mathrm{X}^\intercal \mathrm{X})^{-1} \mathrm{X} \mathrm{y}$.

However, $(\mathrm{X}^\intercal \mathrm{X})^{-1}$ is often poorly conditioned, which leads to a large prediction variance. To solve this issue, a standard technique is to consider the Tikhonov regularization through a $l_2$ penalty.

$$\begin{align}
  \mathcal{L}_{\lambda}=\|\mathrm{y}-\mathrm{X} {\beta}\|_2^2 + \lambda \|\beta\|_2^2.\notag\\
\end{align}$$

The solution is given as follows


$$\begin{align}
  \widehat \beta_{\lambda}=(\mathrm{X}^\intercal \mathrm{X} + \lambda \mathrm{I})^{-1} \mathrm{X} \mathrm{y} \label{ridge_solution}.
\end{align}$$


Increasing $\lambda$ leads to a larger bias but also yields a smaller prediction variance.

### Ridge Regression with $d\gg n$: Minimum-norm estimator {% cite implicit_ridge %}



Taking the limit $\lambda \rightarrow 0$ in Eq.\eqref{ridge_solution}, we have

$$\begin{align}
    \widehat \beta_{0} = \lim_{\lambda \rightarrow 0}\widehat \beta_{\lambda}= \lim_{\lambda \rightarrow 0} \mathrm{V}\left[\dfrac{\mathrm{S}}{\mathrm{S}^2 + \lambda} \right] \mathrm{U}^\intercal \mathrm{y}=\mathrm{V} \mathrm{S}^{-1}\mathrm{U}^\intercal \mathrm{y}=\mathrm{X}^{+}\mathrm{y}.\label{solution}
\end{align}$$

where $\mathrm{X}=\mathrm{U} \mathrm{S} \mathrm{V}^\intercal$ by SVD decomposition, $\mathrm{U}$ and $\mathrm{V}$ are two orthogonal matrices. $\mathrm{X}^{+}=\mathrm{X}^\intercal (\mathrm{X} \mathrm{X}^\intercal)^{-1}$ is the pseudo-inverse of $\mathrm{X}$ (invertible since $\mathrm{X}$ is full rank).


We can easily verify that $\widehat \beta_{0}$ is a solution of $\mathrm{y}-\mathrm{X} {\beta}=0$ because 

$$\begin{align}
    \|\mathrm{y}-\mathrm{X} \widehat{\beta}_0 \|_2 = \| \mathrm{y}-\mathrm{X} \mathrm{X}^{+}\mathrm{y} \|_2=\| \mathrm{y}-\mathrm{y} \|_2=0.\notag
\end{align}$$

Moreover, $\widehat \beta_{0}$ is the minimum-norm estimator in $l_2$

$$\begin{align}
    \widehat \beta_{0} = \text{argmin}_{\beta} \big\{\|\beta\|_2^2 \ \ \big| \ \big\| \mathrm{y}-\mathrm{X} \beta \|_2^2=0 \big\}. \notag
\end{align}$$

Proof:

For any $\beta$ that solves $\mathrm{y}-\mathrm{X} {\beta}=0$, we have $\mathrm{X} \big(\widehat \beta_{0} - \beta \big) = 0.$

Next, we proceed to show $(\widehat \beta_{0} - {\beta})\perp \widehat \beta_{0}$. By Eq.\eqref{solution}, we have 

$$\begin{align}
    \big(\widehat \beta_{0} - {\beta} \big)^{\intercal} \widehat \beta_{0} = \big(\widehat \beta_{0} - {\beta} \big)^{\intercal} \mathrm{X}^\intercal (\mathrm{X} \mathrm{X}^\intercal)^{-1}\mathrm{y} = \big(\mathrm{X}\big(\widehat \beta_{0} - {\beta} \big)\big)^{\intercal} (\mathrm{X} \mathrm{X}^\intercal)^{-1}\mathrm{y}=0.\notag
\end{align}$$

This implies that 

$$\begin{align}
    \| \beta \|_2^2 = \|\beta - \widehat\beta_0 + \widehat \beta_0\|_2^2 = \|\beta - \widehat\beta_0\|_2^2 + \|\widehat \beta_0\|_2^2\geq  \|\widehat \beta_0\|_2^2.\notag
\end{align}$$

This result aligns with Occam's Zazor, suggesting the simplest solution among all the possible alternatives.


#### Tuning of $\lambda$

We study the derivative of the risk function {% cite implicit_ridge %}

$$\begin{align}
    \mathrm{R}(\widehat \beta_{\lambda})=\mathrm{E}\big[((\mathrm{x}^\intercal\beta+\epsilon) - \mathrm{x}^\intercal \widehat\beta_{\lambda})^2 \big]=(\widehat\beta_{\lambda}-\beta)^\intercal \Sigma (\widehat\beta_{\lambda}-\beta)+\sigma^2.\notag
\end{align}$$

We show that the optimal penalty can be $\lambda_{\text{opt}}\leq 0$ by showing $\frac{\partial \mathrm{R}(\widehat \beta_{\lambda})}{\partial \lambda} \bigg\|_{\lambda=0} \leq 0$.

$$\begin{align}
    \frac{\partial \mathrm{R}(\widehat \beta_{\lambda})}{\partial \lambda}=2(\widehat\beta_{\lambda}-\beta)^\intercal \Sigma \frac{\partial \widehat \beta_{\lambda}}{\partial \lambda}.\notag
\end{align}$$

Plugging in the solution \eqref{solution}, we have

$$\begin{align}
    \frac{\partial \mathrm{R}(\widehat \beta_{\lambda})}{\partial \lambda}=2\beta^{\intercal}\Sigma (\mathrm{X}^{\intercal} \mathrm{X})^{+2}\mathrm{X}^\intercal \mathrm{y} - 2\mathrm{y}^\intercal \mathrm{X} (\mathrm{X}^\intercal \mathrm{X})^{+} \Sigma (\mathrm{X}^\intercal \mathrm{X})^{+2} \mathrm{X}^{\intercal} \mathrm{y}\label{relation},
\end{align}$$

where $(\mathrm{X}^{\intercal} \mathrm{X})^{+n}=\mathrm{U} \mathrm{S}^{-n} \mathrm{V}^\intercal$. Applying Eq.\eqref{relation} to the spiked covariance model {% cite implicit_ridge %}, we find that $\frac{\partial \mathrm{E}[\mathrm{R}(\widehat \beta_{\lambda})]}{\partial \lambda} \bigg\|_{\lambda=0} \leq 0$ under some conditions.

<p align="center">
    <img src="/images/negative_ridge_penalty.png" width="200" />
</p>

### Extension to deep neural networks 

Why does deep neural networks generalize so well? The following figure [credit to Belkin] tells us minimizing the smoothness might be the key. 

<p align="center">
    <img src="/images/fit_without_fear.png" width="600" />
</p>

For the details, we refer interested readers to the study in {% cite fit_without_fear %}.]]></content><author><name>Wei Deng</name></author><category term="Regression" /><summary type="html"><![CDATA[Ridge Regression with $n\gg d$: Bias-variance trade-off]]></summary></entry><entry><title type="html">The Triangle of Flow, Diffusion, and PDE</title><link href="http://localhost:4000/posts/flow_diffusion_PDE/" rel="alternate" type="text/html" title="The Triangle of Flow, Diffusion, and PDE" /><published>2023-07-01T00:00:00-07:00</published><updated>2023-07-01T00:00:00-07:00</updated><id>http://localhost:4000/posts/flow-diffusion-PDE</id><content type="html" xml:base="http://localhost:4000/posts/flow_diffusion_PDE/"><![CDATA[<!-- https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference -->

### Diffusion Process

Consider a diffusion process that solves the Itô's SDE {% cite oksendal2003stochastic %}:

$$\begin{align}
\mathrm{d} \mathrm{X}_t=\boldsymbol{\mathrm{v}_t}(\mathrm{X}_t) \mathrm{d} t + \sigma(X_t) \mathrm{d} \mathrm{W}_t.\label{SDE}
\end{align}$$

Denote by $\mathrm{P}_t$ the transition function of Markov process

$$\begin{align}
(\mathrm{P}_t f)(x)=\int f(y)\mathrm{P}(t, x, \mathrm{d} y)\notag.
\end{align}$$

We can easily check that $\mathrm{P}_t$ is a linear operator and an example of a Markov semigroup.

Define the generator $\mathscr{L}f=\lim \frac{\mathrm{P}_t f - f}{y}$, where $\mathrm{P}_t=e^{t \mathscr{L}}$. Analyzing the transition of the conditional expectation $\mathbb{E}(f(\mathrm{X}_t)\|\mathrm{X}_s=x)$ for a bounded function $f$ (Ito's formula), where $s\leq t$, we have the *backward Kolmogorov equation*

$$\begin{align}
\mathscr{L}&=\boldsymbol{\mathrm{v}}\cdot \nabla + \frac{1}{2} \Sigma:D^2\notag\\
           &=\sum_{j=1}^d \mathrm{v}_j\frac{\partial}{\partial x_j} + \frac{1}{2}\sum_{i,j=1}^d \Sigma_{ij}\frac{\partial^2}{\partial x_i \partial x_j}\notag.
\end{align}$$

where $\nabla$ and  $\nabla\cdot$ denote the gradient and the divergence in $\mathbb{R}^d$, $\Sigma(x)=\sigma(x) \sigma(x)^\intercal$ and $D^2$ denotes the Hessian matrix. 


<!-- # https://openreview.net/pdf?id=x9tAJ3_N0k -->

### Fokker-Planck PDE

Further define the semigroup $\mathrm{P}_t^{*}$ that acts on probability measures 

$$\begin{align}
\mathrm{P}^{*}_t \mu(\Gamma)=\int \mathrm{P}(t, x, \Gamma)\mathrm{d} \mu(x)\notag.
\end{align}$$

The semigroup $\mathrm{P}_t$ and $\mathrm{P}_t^{*}$ are adjoint in $L^2$ such that

$$\begin{align}
\int (\mathrm{P}_t f)\mathrm{d}\mu=\int f\mathrm{d}(\mathrm{P}_t^{*}\mu)\notag,
\end{align}$$

which yields

$$\begin{align}
\int \mathscr{L} f h \mathrm{d} x = \int f \mathscr{L}^* h \mathrm{d}x, \text{ where } \mathrm{P}_t^*=e^{t {\mathscr{L}}^*}.\notag
\end{align}$$


Let $p_t$ denote the law of the Markov process at time $t$. The law of $p_t$ follows that

$$\begin{align}
\frac{\partial p_t}{\partial t} =\mathscr{L}^* p_t,\notag
\end{align}$$

which is the Fokker-Planck equation (PDE), also known as *forward Kolmogorov equation*. Analyzing the evolution of the probability densities, we have

$$\begin{align}
\mathscr{L}^{*} p_t&=\nabla \cdot \bigg(-\boldsymbol{\mathrm{v}} p_t + \frac{1}{2} \nabla\cdot\big(\Sigma p_t\big)\bigg). \notag \\
                &=\nabla \cdot \bigg(-\boldsymbol{\mathrm{v}} p_t + \frac{1}{2} \big(\nabla\cdot \Sigma\big) p_t + \frac{1}{2} \Sigma \nabla p_t \bigg). \notag \\
                &=\nabla \cdot \bigg(-\underbrace{\bigg(\boldsymbol{\mathrm{v}} - \frac{1}{2} \big(\nabla\cdot \Sigma\big) - \frac{1}{2} \Sigma \nabla \log p_t\bigg)}_{\boldsymbol{\nu}_t} p_t \bigg), \label{FPE} \\
\end{align}$$

where the last equality follows by $\nabla \log p_t = \frac{\nabla p_t}{p_t}$.




### Probability Flow

Denote by $\boldsymbol{\nu}=\boldsymbol{\mathrm{v}} - \frac{1}{2} \big(\nabla\cdot \Sigma\big) - \frac{1}{2} \Sigma \nabla \log p$, the FPE is recased as the transport equation {% cite OT_applied_math %} or continuity equation in fluid dynamics {% cite log_concave_sampling %}.

$$\begin{align}
\partial_t p_t =- \nabla \cdot ({\boldsymbol{\nu_t}} p_t).\label{ODE}
\end{align}$$

Interestingly, it corresponds to the probability flow ODE {% cite score_sde %}

$$\begin{align}
\mathrm{d} X_t={\boldsymbol{\nu_t}}(X_t) \mathrm{d} t.\notag
\end{align}$$




Apply the instantaneous change of variables {% cite neural_ode %}, the **log-likelihood** of $p_0(x)$ follows that

$$\begin{align}
\log p_0(x)=\log p_T(x) + \int_0^T \nabla \cdot {\boldsymbol{\nu_t}} \mathrm{d} t,\notag
\end{align}$$

which provides an elegant way to compute the likelihood for diffusion models.


In practice, it is expensive to evaluate the divergence $\nabla \cdot {\boldsymbol{\nu_t}}$. We can adopt the Hutchinson trace estimator {% cite Hutchinson89 %}.

$$\begin{align}
\nabla \cdot {\boldsymbol{\nu_t}} = \mathbb{E} \big[\epsilon^\intercal \nabla {\boldsymbol{\nu_t}} \epsilon \big],\notag
\end{align}$$

where $\nabla \cdot {\boldsymbol{\nu_t}}$ is the Jacobian of ${\boldsymbol{\nu_t}}$; the random variable $\epsilon$ is a standard Gaussian vector and $\epsilon^\intercal \nabla {\boldsymbol{\nu_t}}$ can be efficiently computed using reverse-mode automatic differentiation.



#### Wasserstein Gradient Flow

Consider a homogeneous case where $\boldsymbol{\mathrm{v_t}}\equiv -\nabla \mathrm{V}$ and $\Sigma(x)=2\boldsymbol{\text{I}}$ for Eq.\eqref{SDE}, the vector field $\boldsymbol{\nu}$ can be interpreted as the *tangent vector* for the curves of measures $t\rightarrow p_t$ {% cite JKO98 %} {% cite log_concave_sampling %}. Define a functional $\mathcal{F}=\text{KL}(\cdot\|\|\pi)$, where $\pi\propto \exp(-\mathrm{V})$. We have

$$\begin{align}
\mathcal{F}(p)=\int p \log \frac{p}{\pi} = \int p \mathrm{V} + \int p \log p.\notag
\end{align}$$

Taking the first variation of $\mathcal{F}$ at $p$, we have

$$\begin{align}
\delta \mathcal{F}(p)= \mathrm{V} + \log p+\text{constant}.\notag
\end{align}$$

The Wasserstein gradient at $p$ follows that

$$\begin{align}
\nabla_{\text{W}_2} \mathcal{F}(p):=\nabla \delta \mathcal{F}(p)= \nabla \mathrm{V} + \nabla\log p=-\boldsymbol{\nu},\notag
\end{align}$$

where the last equality follows by Eq.\eqref{FPE}.

Now the transport equation \eqref{ODE} can be also formulated as the Wasserstein gradient flow of $\mathcal{F}$ 

$$\begin{align}
\partial_t p_t =\nabla \cdot \bigg(\nabla_{\text{W}_2} \mathcal{F}(p_t) p_t\bigg).\notag
\end{align}$$



The following is a demo that describes the connections:

<p align="center">
    <img src="/images/ODE_PDE_SDE.png" width="300" />
</p>]]></content><author><name>Wei Deng</name></author><category term="Diffusion" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Coupling by Reflection (II)</title><link href="http://localhost:4000/posts/coupling_by_reflection/" rel="alternate" type="text/html" title="Coupling by Reflection (II)" /><published>2023-05-20T00:00:00-07:00</published><updated>2023-05-20T00:00:00-07:00</updated><id>http://localhost:4000/posts/Coupling-by-Reflection</id><content type="html" xml:base="http://localhost:4000/posts/coupling_by_reflection/"><![CDATA[### Limitations of Synchronous Coupling

Given a $\kappa$-strongly convex drift $U$, we can apply the synchronous coupling for the diffusion process

$$\begin{align}
  \mathrm{d}X_t = U(X_t)\mathrm{d}t+\mathrm{d}W_t\notag\\
  \mathrm{d}Y_t = U(Y_t)\mathrm{d}t+\mathrm{d}W_t.\notag\\
\end{align}$$

Eliminating the Brownian motion, we obtain a contractivity property

$$\begin{align}
  \|X_t-Y_t\|\leq \|X_0-Y_0\|^2 \exp(-\kappa t).\notag
\end{align}$$

However, we cannot easily obtain the desired contraction when $U$ is not strongly convex. To address this issue, one should consider a more general coupling method based on a specic metric instead of the standard Euclidean metric. The diffusions may not contract almost surely, but rather in the average sense.

### Reflection Coupling

Define the coupling time $T_c=\inf \\{ t\geq 0  \| X_t =Y_t \\}$. By definition, we know that $X_t=Y_t$ for $t\geq T_c$ {% cite mufa_chen %} {% cite reflection_coupling %} {% cite reflection_coupling_2 %}  {% cite coupling_hmc %}. When the drift term $U$ is zero, we observe that $\\|X_t-Y_t\\|$ remains fixed for any $t$ and synchronous coupling doesn't induce any contraction. 

Let's explore an alternative coupling where the Brownian motion moves in the opposite direction. We anticipate with some probability the processes will merge [Why?].

$$\begin{align}
  \mathrm{d}X_t &= U(X_t)\mathrm{d}t+\mathrm{d}W_t\notag\\
  \mathrm{d}Y_t &= U(Y_t)\mathrm{d}t+(\mathrm{I} - 2\cdot e_t e_t^{\intercal})\mathrm{d}W_t,\notag\\
\end{align}$$

where $e_t=\mathbb{I}\_{[X_t\neq Y_t]}\cdot \frac{X_t-Y_t}{\\|X_t-Y_t\\|}$ and one can identify that $\widetilde W_t=\int_0^t \big[\mathrm{I} - 2\cdot e_s e_s^{\intercal} \big]\mathrm{d} s$ is also a Brownian motion. In addition, $e_t e_t^{\intercal}$ is the orthogonal projection onto the unit vector $e_t$ [\[Hint\]](https://textbooks.math.gatech.edu/ila/projections.html) and you can easily check that $e_t$ is the eigenvector of $\mathrm{I} - 2\cdot e_t e_t^{\intercal}$ with one eigenvalue $-1$..


### Supermartingales 

We first show $\exp(c\cdot t)f(G_t)$ is a supermartingale, where $G_t=\\|X_t-Y_t\\|$.

Apply Ito's lemma to $f(G_t)$, where $f$ is a concave function to induce a new distance metric $d_f(X, Y)=f(\\|X-Y\\|)$ {% cite reflection_coupling %}.

$$\begin{align}
  \mathrm{d} f(G_t)=2f'(R_t)\mathrm{d}W_t+\bigg\{f'(G_t)\cdot \bigg\langle U(X_t)-U(Y_t), \frac{X_t-Y_t}{\|X_t-Y_t\|}\bigg\rangle +2f''(G_t)\bigg\} \mathrm{d}t.\notag
\end{align}$$

Assume $\langle U(X_t)-U(Y_t), X_t-Y_t\rangle \leq -\kappa(r) \frac{\\|X_t-Y_t\\|^2}{2}$, where $\kappa(r)$ is not necessarily positive

$$\begin{align}
  \bigg\langle U(X_t)-U(Y_t), \frac{X_t-Y_t}{\|X_t-Y_t\|}\bigg\rangle \leq -\frac{1}{2} \cdot G_t \cdot\kappa(G_t). \notag
\end{align}$$

Further including the integration factor $\exp(c\cdot t)$, we have

$$\begin{align}
  \dfrac{\mathrm{d} \bigg[\exp(c\cdot t)f(G_t)\bigg]}{\exp(c\cdot t)}\leq 2f'(R_t) \mathrm{d}W_t + \bigg[-\frac{1}{2} G_t \cdot\kappa(G_t) f'(G_t)+2f''(G_t)+c \cdot f(G_t)\bigg]\mathrm{d}t. \notag
\end{align}$$

In other words, it induces a supermartingale when we have

$$\begin{align}
-\frac{1}{2} G_t \cdot\kappa(G_t) f'(G_t)+2f''(G_t)+c \cdot f(G_t)\leq 0.\notag
\end{align}$$


It implies that a proper $f$ may help us obtain the desired result

$$\begin{align}
  \mathrm{E}[f(\|X_t-Y_t\|)] \leq f(\|X_0-Y_0\|)\cdot \exp(-c\cdot t).\notag
\end{align}$$



### How to build such an $f$

#### A simple case when $c = 0$

We propose to find a $f$ that satisfies 

$$\begin{align}
f''(G_t)\leq \frac{1}{4} G_t \cdot\kappa(G_t) f'(G_t).\notag
\end{align}$$

The worst case is given by $f(R)=\int_0^{R} f'(s) \mathrm{d}s$, where $f'$ is solved by Growall inequality

$$\begin{align}
f'(R)&=\exp\bigg\{\int_0^R\frac{1}{4} s \cdot\kappa(G_t) \mathrm{d}s\bigg\}.\notag
\end{align}$$

#### Extention to $c>0$

We aim to obtain the following dimension-independent bound in $R, L\in [0, \infty)$ {% cite reflection_coupling %}.

The general idea is to permit strong convexity outside of a ball with a given radius, within which local non-convexity is allowed.

$-\mathbb{I}\_{[\\|X_t-Y_t\\|< R]} L{\\|X_t-Y_t\\|^2}\leq \langle U(X_t)-U(Y_t), X_t-Y_t\rangle \leq \mathbb{I}\_{[\\|X_t-Y_t\\|\geq R]} K{\\|X_t-Y_t\\|^2}.$]]></content><author><name>Wei Deng</name></author><category term="Coupling" /><summary type="html"><![CDATA[Limitations of Synchronous Coupling]]></summary></entry><entry><title type="html">Girsanov and MLE</title><link href="http://localhost:4000/posts/Girsanov_MLE/" rel="alternate" type="text/html" title="Girsanov and MLE" /><published>2023-03-20T00:00:00-07:00</published><updated>2023-03-20T00:00:00-07:00</updated><id>http://localhost:4000/posts/Girsanov-Likelihood</id><content type="html" xml:base="http://localhost:4000/posts/Girsanov_MLE/"><![CDATA[### Maximum likelihood estimation

Given $N$ independent observations, the likelihood function given $\theta=(\mu, \sigma)$ follows that

$$\begin{align}
\mathcal{L}(\{x_i\}_{i=1}^N|\theta)=\prod_{i=1}^N f(x_i|\theta),\notag
\end{align}$$

which models the density of the random variable $X$. The maximum likelihood estimator (MLE) is given by

$$\begin{align}
\widehat\theta=\text{argmax} \mathcal{L}(\mathbf{x}|\theta),\notag
\end{align}$$

where $\mathbf{x}=\\{x_i\\}_{i=1}^N$. When $X$ is a Gaussian random variable that follows $X\sim \mathcal{N}(\mu, \sigma^2)$. The likelihood function is expressed as

$$\begin{align}\label{MLE}
\mathcal{L}(\{x_i\}_{i=1}^N|\theta)=\bigg(\frac{1}{\sqrt{2\pi \sigma^2}}\bigg)^{N/2} \exp\bigg(-\frac{\sum_{i=1}^N (x_i-\mu)^2}{2\sigma^2}\bigg).
\end{align}$$

Taking the gradient w.r.t. $\mu$ and $\sigma^2$, we have

$$\begin{align}
\widehat \mu=\frac{1}{N}\sum_{i=1}^N x_i, \quad \widehat\sigma^2=\frac{1}{N} \sum_{i=1}^N (x_i-\widehat \mu)^2.\notag
\end{align}$$


### Girsanov theorem

Assume we have a diffusion process

$$\begin{align}
\mathrm{d}X_t = b(X_t;\theta)\mathrm{d}t+\sigma\mathrm{d}W_t.\notag
\end{align}$$

We observe the whole path of the process $X\_t$ from time $[0, T]$. Denote by $\mathbb{P}\_X$ the law of the process on the path space, which is absolutely continuous w.r.t. the Wiener measure. The density of $\mathbb{P}\_X$ w.r.t. the Wiener measure is determined by the Radon-Nikodym derivative 

$$\begin{align}\label{girsanov}
\frac{\mathrm{d}\mathbb{P}_X}{\mathrm{d}\mathbb{P}_W}=\exp\bigg(\frac{1}{\sigma}\int_0^T b(X_s; \theta)\mathrm{d}W_s-\int_0^T \frac{b^2(X_s;\theta)}{2\sigma^2}\mathrm{d}s \bigg).
\end{align}$$




We can observe a close connection between Eq.\eqref{MLE} and a discrete variant of Eq.\eqref{girsanov} 

Remark: While the Girsanov theorem is commonly employed, it is prone to mistakes that can be easily made. Have I made any such errors?


### Applications

Given a stationary Ornstein-Uhlenbeck process, how do you estimate the parameters using MLE {% cite Grigorios_14 %}..

$$\begin{align}
\mathrm{d}X_t = -\alpha X_t \mathrm{d}t+\sigma\mathrm{d}W_t.\notag
\end{align}$$

Hint: 1) write the likelihood; 2) take the gradient.]]></content><author><name>Wei Deng</name></author><category term="Diffusion" /><summary type="html"><![CDATA[Maximum likelihood estimation]]></summary></entry><entry><title type="html">Schrödinger Bridge Problem</title><link href="http://localhost:4000/posts/Bridge/" rel="alternate" type="text/html" title="Schrödinger Bridge Problem" /><published>2022-06-19T00:00:00-07:00</published><updated>2022-06-19T00:00:00-07:00</updated><id>http://localhost:4000/posts/Bridges</id><content type="html" xml:base="http://localhost:4000/posts/Bridge/"><![CDATA[The classical Schrödinger Bridge Problem (SBP) have found interesting applications in deep generative models {% cite Nea11 %}, {% cite forward_backward_SDE %}, {% cite DSB %} and financial mathematics {% cite nutz_trading %} {% cite nutz_portfolio %} {% cite schrodinger_vol_model %}. The iterative nature in solving this problem shows a great potential to further accelerate the training for score-based generative models, although the later is already the state-of-the-art methods [\[Image Generation\]](https://paperswithcode.com/sota/image-generation-on-cifar-10).

The most striking feature of this algorithm is its deep mathematical connections to stochastic optimal control, optimal transport, fluid dynamics {% cite siam_review_Sinkhorn_liaisons %}, and DNN approximations. As such, we have sufficient tools to understand the underlying theories. We refer interested readers to the Appendix A of {% cite provably_schrodinger_bridge %}.]]></content><author><name>Wei Deng</name></author><category term="Diffusion" /><summary type="html"><![CDATA[The classical Schrödinger Bridge Problem (SBP) have found interesting applications in deep generative models (Neal, 2011), (Chen et al., 2022), (De Bortoli et al., 2021) and financial mathematics (Nutz, 2022) (Nutz, 2022) (Henry-Labordere, 2019). The iterative nature in solving this problem shows a great potential to further accelerate the training for score-based generative models, although the later is already the state-of-the-art methods [Image Generation].]]></summary></entry><entry><title type="html">Hamiltonian Monte Caro</title><link href="http://localhost:4000/posts/Hamiltonian/" rel="alternate" type="text/html" title="Hamiltonian Monte Caro" /><published>2021-11-01T00:00:00-07:00</published><updated>2021-11-01T00:00:00-07:00</updated><id>http://localhost:4000/posts/Hamiltonian</id><content type="html" xml:base="http://localhost:4000/posts/Hamiltonian/"><![CDATA[Hamiltonian Monte Carlo {% cite Nea11 %} (HMC) is a popular Markov chain Monte Carlo (MCMC) algorithm to simulate from a probability distribution and is believed to be faster than the Metropolis Hasting algorithm {% cite MRRT53 %} and Langevin dynamics. However, the convergence properties are far less understood compared to its empirical successes. 

In this blog, I will introduce a paper called Optimal Convergence Rate of Hamiltonian Monte Carlo for Strongly Logconcave Distributions {% cite CV19 %}. 

## Hamiltonian Dynamics

HMC algorithms conserves Hamiltonian and the volume in the phase space and enjoy the reversibility condition (by flipping a sign). It aims to simulate particles according to the laws of Hamiltonian dynamics. Consider a Hamiltonian function $H(x, v)$ defined as follows

\begin{equation}\notag
H(x, v)=f(x)+\\|v\\|^2,
\end{equation}

where $f$ is the potential energy function, $x$ is the position, and $v$ is the velocity variable. In each step, the update of the particles $(x, v)$ follows the system of (ordinary) differential equations as follows


\begin{equation}\label{hmc_eq}
\frac{d x}{d t}=\frac{\partial H}{\partial v}=v(t) \ \ \text{and} \ \ \frac{d v}{d t}=-\frac{\partial H}{\partial x}=-\nabla f(x).
\end{equation}

After a time interval $t$, the solutions follow a ``Hamiltonian flow'' $\varphi_t$ that maps $(x,v)$ to $(x_t(x,v), v_t(x, v))$.





## Convergence of Ideal HMC

To prove the convergence of the ideal HMC algorithms, we first assume standard assumptions.

**Strong convexity** We say $f$ is $\mu$-strongly convex if for all $x, y\in R^d$, we have

\begin{equation}\notag
f(y)\geq f(x)+\langle \nabla f(x), y-x\rangle + \frac{\mu}{2} \\|y-x\\|^2.
\end{equation}

**Smoothness** We also assume $f$ is $L$-smooth in the sense that

\begin{equation}\notag
f(y)\leq f(x)+\langle \nabla f(x), y-x\rangle + \frac{L}{2} \\|y-x\\|^2.
\end{equation}

The convergence analysis hinges on the coupling of two Markov chains such that the distance between the position variables $x_k$ and $y_k$ (the second Markov chain) contracts in each step.

Denote by $x(t)$ and $y(t)$ solutions of HMC (\ref{hmc_eq}) and denote by $x(0)$ and $y(0)$ the initial positions of two ODEs for HMC. To faciliate the analysis of coupling techniques, we adopt the same initial velocity $v(0)=u(0)$. The convergence study hinges on the contraction bound as follows

<p align="center">
    <img src="/images/HMC_coupling2.png" width="400" />
</p>


**Lemma** Assume the potential function $f$ satisfies the convexity and smoothness assumptions. Then for $0\leq t \leq \frac{1}{2\sqrt{L}}$, we have

\begin{equation}\notag
\\|x(t)-y(t)\\|^2 \leq (1-\frac{\mu}{4}t^2) \\|x(0)-y(0)\\|^2.
\end{equation}


**Proof**
Consider two ODEs for HMCs: 

$$\begin{align}\notag
\qquad\qquad\qquad\qquad x'(t)&=v(t)    \qquad\qquad \quad\text{and}\qquad y'(t)=u(t) \notag\\\\
\qquad\qquad\qquad\qquad v'(t)&=-\nabla f(x(t))     \quad\qquad\qquad\ \   u'(t)=-\nabla f(y(t)),\notag
\end{align}$$

where the initial velocities follow $u(0)=v(0)$. 

Taking the second derivative of $\frac{1}{2}\\|x-y\\|^2$, we have

$$\begin{align}\notag
\frac{d^2}{dt^2}\left(\frac{1}{2} \|x-y\|^2\right)&=\frac{d}{dt}\langle v-u, x-y \rangle\notag\\
						  &=\langle v'-u', x-y \rangle + \langle v-u, x'-y' \rangle \notag\\
						  &=-\rho \|x-y\|^2 + \|v-u\|^2,\notag
\end{align}$$

where $\rho=\rho(t)=\frac{\langle \nabla f(x) - \nabla f(y), x-y \rangle}{\\|x-y\\|^2}$.

To upper bound $\\|v-u\\|^2$, recall that 
\begin{equation}\notag
\frac{d}{dt} \\|x\\|=\frac{d}{dx} \\|x\\| \cdot \frac{d}{dt} x=\frac{\langle x, \dot{x} \rangle}{\\|x\\|}.
\end{equation}

In what follows, we have

\begin{equation}\notag
\frac{d}{dt}\\|v-u\\|=\frac{1}{\\|v-u\\|}\langle v'-u', v-u\rangle =-\frac{\langle \nabla f(x)-\nabla f(y), v-u\rangle}{\\|v-u\\|}.
\end{equation}

In particular for the upper bound of $\frac{d}{dt}\\|v-u\\|$, we have

$$\begin{align}
\left|\frac{d}{dt}\|v-u\|\right| &\leq \|\nabla f(x)-\nabla f(y)\| \notag\\
                                 & \leq \sqrt{L \langle \nabla f(x) - \nabla f(y), x-y \rangle} \notag\\
                                 & = \sqrt{L\rho \|x-y\|^2} \notag\\
				 & \leq \sqrt{2L\rho \|x_0-y_0\|^2}, \notag\\
\end{align}$$

where the first inequality follows by Cauchy–Schwarz inequality, the second inequality follows by the L-smoothness assumption, and the last inequality follows by Claim 7 in {% cite CV19 %}.

Now, we can upper bound $\\|v-u\\|^2$ as follows

$$\begin{align}
\|v-u\|^2 &\leq  \left(\int_0^t \left|\frac{d}{ds}\|v-u\|\right| ds\right)^2 \notag\\
\qquad\qquad & \leq \left(\int_0^t \sqrt{2 L\rho} \|x_0-y_0\| ds\right)^2 \notag\\
\qquad\qquad & \leq 2L t \left(\int_0^t \rho ds\right) \|x_0 - y_0\|^2. \notag\\
\end{align}$$

Define the monotone increasing function

\begin{equation}\notag
P=P(t)=\int_0^t \rho dt,
\end{equation}

where $P(0)=0$ and $\mu t \leq P(t)\leq L t$ for all $t\geq 0$. Then

\begin{equation}\notag
\|v-u\|^2 \leq 2L t P\|x_0-y_0\|^2.
\end{equation}

Combining the above upper bounds, we have

\begin{equation}\notag
\frac{d^2}{dt^2} \left(\frac{1}{2}\\|x-y\\|^2 \right)\leq -\rho \left(\frac{1}{2} \\|x_0-y_0\\|^2\right)+2Lt P \\|x_0-y_0\\|^2.
\end{equation}

Define $\alpha(t)=\frac{1}{2} \\|x-y\\|^2$, then we have

\begin{equation}\notag
\alpha'(t)\leq -\alpha(0) (\rho(t)-4L t P(t)).
\end{equation}

Integrating both sides and combining the fact that $\alpha'(0)=0$, we have

$$\begin{align}
\alpha'(t)&=\int_0^t \alpha'(s) ds \notag\\
\qquad\ &\leq -\int_0^t \alpha(0) (\rho(s)-4L t P(s))ds \notag\\
\qquad\ &\leq -\alpha(t)\left(P(t) - 4LP(t) \int_0^t s ds\right) \notag\\
\qquad\ &=-\alpha(0)P(t)(1-2Lt^2). \notag
\end{align}$$

Choose $t \in [0, \frac{1}{2\sqrt{L}}]$, then we can deduce that
\begin{equation}\notag
\alpha'(t)\leq -\alpha(0) \frac{\mu}{2} t.
\end{equation}

Eventually, we have the desired result such that

\begin{equation}\notag
\alpha(t)=\alpha(0)+\int_0^t \alpha'(s) d s \leq \alpha(0) \left(1-\frac{\mu}{4} t^2\right).
\end{equation}


Setting $t=T=1/(c\sqrt{L})$ for some constant $c\geq 2$, we get

\begin{equation}\notag
\|x(T)-y(T)\|^2 \leq \left(1-\frac{1}{4c^2 \kappa}\right)\|x(0)-y(0)\|^2.
\end{equation}

Eventually, we can achieve the convergence in Wasserstein distance such that 

\begin{equation}\notag
W^2_2(\nu_k, \pi)\leq E\left[\|x_k-y_k\|^2\right]\leq \left(1-\frac{1}{4c^2 \kappa}\right)^k O(1).
\end{equation}

## Discretized HMC

{% cite MV18 %} proposed to approximate the Hamiltonian trajectory with a second-order Euler integrator such that

\begin{equation}\notag
\hat{x_{\eta}}(x, v)=x+v \eta - \frac{1}{2} \nabla f(x), \quad \hat{v_{\eta}}(x, v)=v-\eta \nabla f(x) - \frac{1}{2} \eta^2 \nabla^2 f(x) v.
\end{equation}

Since Hessian is expensive to computate and store, an approximation is conducted through

\begin{equation}\notag
\nabla^2 f(x) v \approx \frac{\nabla f(\hat{x_{\eta}}) - \nabla f(x)}{\eta},
\end{equation}

Now, the numerical integrator follows that

\begin{equation}\notag
\hat{x_{\eta}}(x, v)=x+v \eta - \frac{1}{2} \nabla f(x), \qquad \hat{v_{\eta}}(x, v)=v-\frac{1}{2}\eta (\nabla f(x) - \nabla f(\hat{x_{\eta}})).
\end{equation}

It can be shown that such a discretized HMC requires O($d^{\frac{1}{4}} \epsilon^{-\frac{1}{2}}$) gradient evaluations under proper regularity assumptions {% cite MV18 %}. Other interesting persepectives can be seen in {% cite Che20 %}.



## Conclusions

Properly tuning the number of leapfrog steps is important for maximizing the contraction to accelerate convergence. From the perspective of methodology, the no-U-turn sampler proposes to automatically adjust the number of leapfrog steps and potentially exploits this idea by checking the inner product of postion and velocity {% cite HG14 %}.]]></content><author><name>Wei Deng</name></author><category term="Sampling" /><summary type="html"><![CDATA[Hamiltonian Monte Carlo (Neal, 2011) (HMC) is a popular Markov chain Monte Carlo (MCMC) algorithm to simulate from a probability distribution and is believed to be faster than the Metropolis Hasting algorithm (Metropolis, 1953) and Langevin dynamics. However, the convergence properties are far less understood compared to its empirical successes.]]></summary></entry><entry><title type="html">Couplings and Monte Carlo Methods (I)</title><link href="http://localhost:4000/posts/Couplings/" rel="alternate" type="text/html" title="Couplings and Monte Carlo Methods (I)" /><published>2021-08-01T00:00:00-07:00</published><updated>2021-08-01T00:00:00-07:00</updated><id>http://localhost:4000/posts/Coupling</id><content type="html" xml:base="http://localhost:4000/posts/Couplings/"><![CDATA[A coupling of two random variables/ vectors represents a joint distribution, where the marginal distributions are denoted by $p$ and $q$, respectively. Any joint distributions of $p$ and $q$ define a valid coupling. Although there are infinitely many of them, some of them are quite special and may facilitate our analysis. 


# Maximal Coupling

Total variation distance is defined as follows

\begin{equation}\notag
\forall A\in X, \quad \|p-q\|\_{\text{TV}}=\sup\_{X\in A} \mathbb{P}(X\in A) - \mathbb{P}(Y\in A),
\end{equation}
where $X$ and $Y$ are random variables defined on the state space $\mathbb{X}$ with measurable set $X$. Note that for any measurable set $A\in X$, the follow inequality always holds

$\mathbb{P}(X\in A)- \mathbb{P}(Y\in A)=\mathbb{P}(X\in A \cap X\neq Y)- \mathbb{P}(Y\in A \cap X\neq Y)\leq \mathbb{P}(X\neq Y)$.

There exists one pair of $(X,Y)$ such that the above inequality holds. Any coupling that satisfies this property is known as **maximal coupling**.

Denoting by $\nu=p-q$ the zero-mass measure, there is a set $D$ according to the Hahn-Jordan decomposition such that $\nu^{+}=(p-q)(\cdot \cap D)$ and $\nu^{-}=(q-p)(\cdot \cap D^c)$ are both positive measures and $\nu=\nu^{+}-\nu^-$.


Notably, it is clear that $D=\\{x:\ p(x)\geq q(x)\\}$. It follows that $\sup_{A\in {X}} \mathbb{P}(X\in A)- \mathbb{P}(Y\in A)=\sup_{A\in {X}}\nu(A)=\nu(D)=\mathbb{P}(X\in D)-\mathbb{P}(Y\in D)$. We have

$$\begin{align}
\mathbb{P}(X\in D)-\mathbb{P}(Y\in D)&=\int_{\{x:\ p(x)\geq q(x)\}} p(x) d x - \int_{\{x:\ p(x) \geq q(x)\}} q(x) d x\nonumber\\
&=\int_{\{x:\ p(x)< q(x)\}} q(x) d x - \int_{\{x:\ p(x) < q(x)\}} p(x) d x.\nonumber
\end{align}$$

<p align="center">
    <img src="/images/TS_distance.png" width="600" />
</p>
Summing up the above equations and combining $|a-b|=a+b-2 a\wedge b$, we can obtain the area between the two pdf curves {% cite Jacob_Pierre %}

\begin{equation}\notag
\|p-q\|_{\text{TV}}=\frac{1}{2} \int |p(x)- q(x)| d x = 1-\int \min \\{p(x), q(x) \\} dx.
\end{equation}
 
In the end, we can summarize the different formulations of total variation
$$\begin{align}\notag
\|p-q\|_{\text{TV}}&=\sup \mathbb{P}(X\in A)- \mathbb{P}(Y\in A)\\
		   &=\frac{1}{2}\int |p(x)- q(x)| d x\notag\\
		   &=1-\int \min\{p(x), q(x)\} d x\notag\\
		   &=\inf_{(X,Y)\in \text{coupling}(p, q)} \mathbb{P}(X\neq Y).\notag
\end{align}$$


### Applications

Suppose $(X_i)$ follow an (independent) binomial distribution $Binomial(\lambda, N)$ and $(Y_i)$ follow an (independent) Poisson distribution $Poisson(\lambda)$. Next, we couple $X_i$ and $Y_i$ maximally. The **overlap mass** is equal to $(1-\lambda)\wedge e^{-\lambda} + \lambda \wedge \lambda e^{-\lambda}=1-\lambda+\lambda e^{-\lambda}$. That means $\mathbb{P}(X_i\neq Y_i)=\lambda(1-e^{-\lambda})\leq \lambda^2$.

Replace $\lambda$ with $\lambda/N$, we have
\begin{equation}\notag
\mathbb{P}\left(\frac{\sum_{i=1}^N X_i-\sum_{i=1}^N Y_i}{N}\right)\leq \frac{\lambda^2}{N},
\end{equation}
where implies that given a large N and a fixed $\lambda$, Poisson distribution approximates the binomial distribution.

### Convergence rates

By assuming $(Y_t)$ is simulated from the stationary distribution in the beginning, we can introduce a coupling inequality such that

\begin{equation}\notag
{L}(X_t)-{L}(Y_t)\| \leq \mathbb{P}(\tau>t),
\end{equation}
where $\tau$ is a random variable that enables $X_t=Y_t$ and is also known as ''meeting time''. The equality can be achieved given the maximal coupling but the contruction may differ in different problems. In addition, meeting exactly sometimes is quite restricted; it is enough to consider close enough chains.

# Synchronous Coupling

Synchronous coupling models the contraction of a pair of trajectories and can be used to prove the convergence of stochastic gradient Langevin dynamics for strongly log-concave distributions.

Let $\theta_k\in \mathbb{R}^d$ be the $k$-th iterate of the following stochastic gradient Langevin algorithm.
\begin{align}\notag
    \theta_{k+1}=\theta_k -\eta \nabla \widetilde f(\theta_k)+\sqrt{2\tau\eta}\xi_k,
\end{align}
where $\eta$ is the learning rate, $\tau$ is the temperature, $\xi_k$ is a standard $d$-dimensional Gaussian vector, and $\nabla \widetilde f(\theta)$ is an unbiased estimate of the exact gradient $\nabla f(\theta)$.




## Assumptions

**Smoothness** We say $f$ is $L$-smooth if for some $L>0$
\begin{align}\label{def:strong_convex}
f(y)\leq f(x)+\langle \nabla f(x),y-x \rangle+\frac{L}{2}\\| y-x \\|^2_2\quad \forall x, y\in \mathbb{R}^d.
\end{align}


**Strong convexity**
We say $f$ is $m$-strongly convex if for some $m>0$
\begin{align}\label{def:smooth}
f(x)\geq f(y)+\langle \nabla f(y),x-y \rangle + \frac{m}{2} \\| y-x \\|_2^2\quad \forall x, y\in \mathbb{R}^d.
\end{align}


**Bounded variance** The variance of stochastic gradient $\nabla \widetilde f(x)$ is upper bounded such that
\begin{align}\label{def:variance}
\mathbb{E}[\\|\nabla \widetilde f(x)-\nabla f(x)\\|^2] \leq \sigma^2 d,\quad \forall x\in \mathbb{R}^d.
\end{align}




## A Crude Estimate

Assume assumptions \ref{def:strong_convex}, \ref{def:smooth}, and \ref{def:variance} hold. For any learning rate $\eta \in (0 , 1 \wedge {m}/{L^2} )$  and $\|\| \theta_0-\theta_* \|\| \leq \sqrt{d} {D}$, where $\theta_*$ is a stationary point. Then


\begin{align}\notag
W_2(\mu_k, \pi) \leq e^{-{mk\eta}/{2}} \cdot 2 ( \sqrt{d} {D} + \sqrt{d/m} ) + \sqrt{ 2d (\sigma^2+L^2 G\eta) / m^2},
\end{align}

where $\mu_k$ denotes the probability measure of $\theta_k$ and $G:=25(\tau+m {D}^2+\sigma^2)$.


## Proof
Denote $\theta_t$ as the continuous-time interpolation of the stochastic gradient Langevin dynamics as follows

\begin{align}\notag
d \theta_t = - \nabla \widetilde f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) d t + \sqrt{2\tau} d \widetilde W_t,
\end{align}

where ${\theta}_0=\theta_0$. For any $k\in \mathbb{N}^{+}$ and a time $t$ that satisfies $t=k\eta$, it is apparent that $\widehat\mu_t={L}({\theta}_t)$ is the same as $\mu_k={L}(\theta_k)$, where ${L}(\cdot)$ denotes a distribution of a random variable. In addition, we define an auxiliary process $(\beta_t)$ that starts from the stationary distribution $\pi$

\begin{align}\notag
d \beta_t = - \nabla f(\beta_t) d t + \sqrt{2\tau} d W_t.
\end{align}



Consider Itô's formula for the sequence of $\frac{1}{2}  \\| \theta_t - \beta_t \\| ^2$

$$\begin{align}\notag
&\ \ \ \frac{1}{2} d  \| \theta_t - \beta_t \|^2\notag\\
&= \langle \theta_t - \beta_t, d \theta_t - d \beta_t \rangle + \mathrm{Tr}[ d^2 \theta_t - d^2 \beta_t ]\notag\\
&=\langle \theta_t - \beta_t, (\nabla f(\beta_t) -\nabla\widetilde  f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) ) d t + \sqrt{2\tau} ( d \widetilde W_t - d W_t ) \rangle + 2\tau \mathrm{Tr}[ d^2 \widetilde W_t - d^2 W_t ].\notag
\end{align}$$


Taking $\widetilde W_t=W_t$ defines a synchronous coupling. Arrange the terms

$$\begin{align}\notag
&\ \ \ \frac{1}{2} d \| \theta_t - \beta_t \|^2 \notag\\
&= \langle \theta_t - \beta_t, \nabla f(\beta_t)-\nabla f(\theta_t) \rangle d t+ \langle \theta_t - \beta_t,  \nabla f(\theta_{t}) - \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor})  \rangle d t\notag\\
&\qquad \qquad\qquad+ \langle \theta_t - \beta_t, \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) - \nabla \widetilde f(\widehat \theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \rangle d t \notag\\
&\leq - m \| \theta_t - \beta_t \|^2 d t + \frac{m}{4} \| \theta_t - \beta_t \|^2 d t + \frac{1}{m}  \| \nabla f(\theta_{t}) - \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor})  \|^2 d t \notag\\
&\qquad\qquad  + \frac{m}{4} \| \theta_t - \beta_t \|\|^2 d t + \frac{1}{m} \| \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) - \nabla \widetilde f(\widehat \theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \|^2 d t\notag\\
&\leq  - \frac{m}{2} \| \theta_t - \beta_t \|\|^2 d t + \frac{d\sigma^2}{m} d t +\frac{1}{m} \| \nabla f(\theta_{t}) - \nabla f(\theta_{\eta\lfloor\frac{t}{\eta} \rfloor}) \|^2  d t\notag\\
&\leq  - \frac{m}{2} \| \theta_t - \beta_t \|^2 d t + \frac{d\sigma^2}{m} d t+ \frac{L^2}{m} \| \theta_{t} - \theta_{\eta\lfloor\frac{t}{\eta} \rfloor} \|^2  d t\notag\\
\end{align}$$


where the first inequality follows from $ab\leq  (\frac{\sqrt{m}}{2}a)^2+({\frac{1}{\sqrt{m}}}b)^2$ and the strong-convexity property \ref{def:strong_convex}; in particular, we don't attempt to optimize the constants of $-\frac{m}{2}$ for the item $\|\| \theta_t - \beta_t \|\|^2$; the second and third inequalities follow by bounded variance assumption \ref{def:variance} and the smoothness assumption \ref{def:smooth}, respectively.


Now apply Grönwall's inequality to the preceding inequality and take expectation respect to a coupling $(\theta_t, \beta_t) \sim \Gamma(\widehat\mu_t,\pi)$
\begin{align}\label{eq:1st_gronwall}
     \mathbb{E}{ \\|\theta_t - \beta_t \\|^2}\leq  \mathbb{E}{\\| \theta_0 - \beta_0 \\|^2} e^{-mt}+\frac{2}{m}\int_0^t g(d\sigma^2+ L^2\underbrace{\mathbb{E}{ \\| \theta_{s} - \theta_{\eta\lfloor\frac{s}{\eta} \rfloor} \\|^2 }}_{I: \text{Discretization error}} g) e^{-(t-s)m} d s. 
\end{align}


Plugging some estimate of $I$ into Eq.\eqref{eq:1st_gronwall}, we have
$$\begin{align}\notag
&\ \ \ \mathbb{E}{ \| \theta_t - \beta_t \|^2 }\\
&\leq  \mathbb{E}{ \| \theta_0 - \beta_0 \|^2} e^{-mt}+\frac{2d}{m} (\sigma^2+L^2 G\eta) \int_0^t  e^{-(t-s)m} d s\notag\\
&\leq \mathbb{E}{ \| \theta_0 - \beta_0 \|^2} e^{-mt}+\frac{2d}{m^2} (\sigma^2+L^2 G\eta).\notag
\end{align}$$


Recall that $\theta_k$ and $\widehat\theta_{t\eta}$ have the same distribution $\mu_k$. By the definition of $W_2$ distance, we have

$$\begin{align}\notag
&\ \ \ W_2(\mu_k, \pi)\\
&\leq e^{-{mk\eta}/{2}} \cdot W_2(\mu_0, \pi) + \sqrt{ 2d (\sigma^2+L^2 G\eta) / m^2}\notag\\
&\leq e^{-{mk\eta}/{2}} \cdot 2 (\| \theta_0 - \theta_* \| +  \sqrt{d/m} )+ \sqrt{ 2d(\sigma^2+L^2 G\eta) / m^2}\notag\\
&\leq e^{-{mk\eta}/{2}} \cdot 2 ( \sqrt{d} {D} +  \sqrt{d/m} )+  \sqrt{ 2 d(\sigma^2+L^2 G\eta) / m^2},\notag
\end{align}$$

where the first inequality follows by applying $\sqrt{\| a + b \|} \leq \sqrt{\| a \|} + \sqrt{\| b \|}$, the second one follows by an estimate of $W_2(\mu_0, \pi)$, and the last step follows from the initialization condition.

### Remark: How to achieve a sharper upper bound?]]></content><author><name>Wei Deng</name></author><category term="Coupling" /><summary type="html"><![CDATA[A coupling of two random variables/ vectors represents a joint distribution, where the marginal distributions are denoted by $p$ and $q$, respectively. Any joint distributions of $p$ and $q$ define a valid coupling. Although there are infinitely many of them, some of them are quite special and may facilitate our analysis.]]></summary></entry></feed>