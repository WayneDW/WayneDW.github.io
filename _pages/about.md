---
permalink: /
title: "Wei Deng"
excerpt: "what is this?"
author_profile: true
---
<p align="center"> <img class="img-circle img-200" width="100%" src="images/lake_tahoe.png"></p>


I am a Researcher at Morgan Stanley [Link](https://www.morganstanley.com/about-us/technology/machine-learning-research-team). I do 70% time on financial data and 30% on research. My interest is to study GPU-friendly sampling, diffusion, filtering, and transport methods. The applications include Bayesian uncertainty, generative models, fintech, and computer vision. I actively contribute to open-source projects such as [Contour Sampler in JAX](https://github.com/blackjax-devs/blackjax) and [NLP in Finance](https://github.com/WayneDW/Sentiment-Analysis-in-Event-Driven-Stock-Price-Movement-Prediction). 

<!--- Selected Publications I am furtunate to work with: [Shikai Fang](https://users.cs.utah.edu/~shikai/), [Yixin Tan](https://scholar.google.com/citations?user=3AGaybIAAAAJ&hl=zh-CN), [Kevin Rojas](https://kevinrojas1499.github.io/) during the summer internships since 2022. -->



I got my Ph.D. in applied math at Purdue University in Dec 2021, where I was advised by Prof. [Guang Lin](https://www.math.purdue.edu/~lin491/) and <a href="https://www.stat.purdue.edu/~fmliang/" target="_blank">Faming Liang</a>. My thesis is: <a href="https://hammer.purdue.edu/articles/thesis/Non-convex_Bayesian_Learning_via_Stochastic_Gradient_Markov_Chain_Monte_Carlo/17161718" target="">Non-convex Bayesian Learning via Stochastic Gradient MCMC</a>



Contact: firstnamelastname056@gmail.com


<ul class="pub-links"> <li> <a href="https://scholar.google.com/citations?user=IYiyxssAAAAJ&hl=en" target="_blank">Scholar</a> </li> <li> <a href="https://github.com/WayneDW" target="_blank">Github</a> </li>  <li> <a href="https://twitter.com/dwgreyman" target="_blank">Twitter</a> </li>   <li> <a href="https://openreview.net/profile?id=~Wei_Deng1" target="_blank">OpenReview</a> </li>   </ul>  


News
=====

Dec, 2023. Glad to participate [Transport, Diffusion, and Sampling Workshop](https://events.simonsfoundation.org/event/335b8972-e63d-4672-82f3-4dd95868cb40/websitePage:5849ec81-d030-4248-b300-3290be94dbdf?_gl=1*28zgwu*_ga*MzkzMDg3OTI0LjE3MDA5MzkyMjM.*_ga_C1G2F4HXQL*MTcwMTI3NzMxNi4zLjEuMTcwMTI3Nzk1Ni4wLjAuMA..)

Nov, 2023. Talk at [Financial Mathematics Seminar at FSU](https://www.math.fsu.edu/finmath/seminars.php)

Apr, 2023. One ICML on [diffusion Schrodinger bridge with transformers](https://arxiv.org/pdf/2305.07247) is accepted!

Dec, 2022. The [Contour Sampler](https://proceedings.neurips.cc/paper/2020/file/b5b8c484824d8a06f4f3d570bc420313-Paper.pdf) is implemented in [BlackJAX](https://blackjax-devs.github.io/sampling-book/algorithms/contour_sgld.html)! 

Feb, 2022. Talk at [Opt and ML Seminar at HKU](https://hkumath.hku.hk/MathWWW/event/2022/OML-DENG_Wei.pdf)

Oct, 2021. I have defended my thesis!

Oct, 2020. Talk at ML + X Seminar at Brown University




<!--- Selected Publications -->
<!---  ====== -->
<!--- My interests mainly focus on scalable Monte Carlo methods. The research finds extensive applications in uncertainty estimation, feature selection, and reinforcement learning. -->

<!--- ### Monte Carlo and Uncertainty Estimation -->

<!--- * **W. Deng**<sup>*</sup>, Q. Feng<sup>*</sup>, G. Karagiannis, G. Lin, F. Liang. [Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction](https://openreview.net/pdf?id=iOnhIy-a-0n). ICLR 2021. [\[video\]](https://slideslive.com/38954013/accelerating-convergence-of-replica-exchange-stochastic-gradient-mcmc-via-variance-reduction?ref=speaker-30773-latest) -->

<!--- * **W. Deng**, Q. Feng<sup>*</sup>, L. Gao<sup>*</sup>, F. Liang, G. Lin. [Non-convex Learning via Replica Exchange Stochastic Gradient MCMC](https://arxiv.org/pdf/2008.05367.pdf). ICML 2020. [\[slides\]](https://icml.cc/media/Slides/icml/2020/virtual(no-parent)-16-15-00UTC-6023-non-convex_lear.pdf) -->

<!--- * **W. Deng**, G. Lin, F. Liang. [A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions](https://arxiv.org/pdf/2010.09800.pdf). NeurIPS 2020 [\[slides\]](https://github.com/WayneDW/Contour-Stochastic-Gradient-Langevin-Dynamics/blob/master/figures/slides.pdf) [\[video\]](https://slideslive.com/38936402/a-contour-stochastic-gradient-langevin-dynamics-algorithm-for-simulations-of-multimodal-distributions) -->


<!--- Research Talks -->
<!--- ====== -->


<!--- * Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte Carlo. *Ph.D. Oral Defense*. Purdue University. Oct.26, 2021. [\[video\]](https://www.youtube.com/watch?v=MGcg0Wh9K68) -->

<!--- * Accelerating Convergence of Stochastic Gradient MCMC via Replica Exchange and Dynamic Importance Sampling. *Department of Machine Intelligence, Peking University & School of Statistics, Renmin University of China & Machine Learning Research, Morgan Stanley*. 2021 -->


Services
======

Reviewers: ICML, NeurIPS, ICLR, AISTAT, AAAI, IJCAI, Machine Learning Journal.


La pensée n'est qu'un écliar au milieu d'une longue nuit. Mais c'est cet éclair qui est tout.
